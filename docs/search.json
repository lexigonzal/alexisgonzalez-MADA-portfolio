[
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Alexis Gonzalez Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Alexis Gonzalez Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Alexis Gonzalez Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Alexis Gonzalez Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda copy.html",
    "href": "starter-analysis-exercise/code/eda-code/eda copy.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/alexisgonzalez/Desktop/MADA/alexisgonzalez-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Hair Color            0             1   3   6     0        3          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean    sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0  133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2   45  55  70  80  110 ▇▂▃▂▂\n3 Thumb Length          0             1  47.4  8.35  35  42  47  50   62 ▅▅▇▁▅\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\nBelow is a boxplot of hair color v. height\n\np5 &lt;- mydata %&gt;% ggplot(aes(x=`Hair Color`, y=Height)) + geom_boxplot(color=\"firebrick3\") \nplot(p5)\n\n\n\n\n\n\n\nfigure_file=here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-hair-boxplot.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n\n\nBelow is a scatterplot of thumb size v. weight.\n\np6 &lt;- mydata %&gt;% ggplot(aes(x=Weight, y=`Thumb Length`)) + \n  geom_point(size=2, color=\"firebrick3\")\nplot(p6)\n\n\n\n\n\n\n\nfigure_file=here(\"starter-analysis-exercise\",\"results\",\"figures\",\"thumbsize-weight-scatterplot.png\")\nggsave(filename = figure_file, plot=p6) \n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/alexisgonzalez/Desktop/MADA/alexisgonzalez-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Alexis Gonzalez Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Alexis Gonzalez Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "I found this really cool analysis from fivethirtyeight of past bachelor and bachelorette seasons. This analysis consists of a few graphics that show patterns in the bachelor franchise seasons. I’m going to recreate the second graph in this article that shows where the first impression post has led in past seasons. Here is the original graph:\n\n\n\n\n\n\nfirst I will load the necessary packages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggforce)\nlibrary(here)\n\nhere() starts at /Users/alexisgonzalez/Desktop/MADA/alexisgonzalez-MADA-portfolio\n\n\n\n\nNow I will load in the dataset\n\nbach &lt;- read.csv(\"/Users/alexisgonzalez/Desktop/MADA/alexisgonzalez-MADA-portfolio/presentation-exercise/bachelorette/bachelorette.csv\")\n\n\nstr(bach)\n\n'data.frame':   921 obs. of  23 variables:\n $ SHOW          : chr  \"SHOW\" \"Bachelorette\" \"Bachelorette\" \"Bachelorette\" ...\n $ SEASON        : chr  \"SEASON\" \"13\" \"13\" \"13\" ...\n $ CONTESTANT    : chr  \"ID\" \"13_BRYAN_A\" \"13_PETER_K\" \"13_ERIC_B\" ...\n $ ELIMINATION.1 : chr  \"1\" \"R1\" \"\" \"\" ...\n $ ELIMINATION.2 : chr  \"2\" \"\" \"R\" \"\" ...\n $ ELIMINATION.3 : chr  \"3\" \"\" \"\" \"R\" ...\n $ ELIMINATION.4 : chr  \"4\" \"R\" \"\" \"\" ...\n $ ELIMINATION.5 : chr  \"5\" \"R\" \"\" \"\" ...\n $ ELIMINATION.6 : chr  \"6\" \"\" \"R\" \"R\" ...\n $ ELIMINATION.7 : chr  \"7\" \"R\" \"R\" \"R\" ...\n $ ELIMINATION.8 : chr  \"8\" \"\" \"\" \"\" ...\n $ ELIMINATION.9 : chr  \"9\" \"\" \"\" \"E\" ...\n $ ELIMINATION.10: chr  \"10\" \"W\" \"E\" \"\" ...\n $ DATES.1       : chr  \"1\" \"\" \"\" \"\" ...\n $ DATES.2       : chr  \"2\" \"\" \"D1\" \"D10\" ...\n $ DATES.3       : chr  \"3\" \"D6\" \"D6\" \"D8\" ...\n $ DATES.4       : chr  \"4\" \"D13\" \"D13\" \"D13\" ...\n $ DATES.5       : chr  \"5\" \"D1\" \"D9\" \"D9\" ...\n $ DATES.6       : chr  \"6\" \"D7\" \"D7\" \"D1\" ...\n $ DATES.7       : chr  \"7\" \"D1\" \"D1\" \"D3\" ...\n $ DATES.8       : chr  \"8\" \"D1\" \"D1\" \"D1\" ...\n $ DATES.9       : chr  \"9\" \"D1\" \"D1\" \"D1\" ...\n $ DATES.10      : chr  \"10\" \"D1\" \"D1\" \"\" ...\n\nsummary(bach)\n\n     SHOW              SEASON           CONTESTANT        ELIMINATION.1     \n Length:921         Length:921         Length:921         Length:921        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION.2      ELIMINATION.3      ELIMINATION.4      ELIMINATION.5     \n Length:921         Length:921         Length:921         Length:921        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION.6      ELIMINATION.7      ELIMINATION.8      ELIMINATION.9     \n Length:921         Length:921         Length:921         Length:921        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION.10       DATES.1            DATES.2            DATES.3         \n Length:921         Length:921         Length:921         Length:921        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   DATES.4            DATES.5            DATES.6            DATES.7         \n Length:921         Length:921         Length:921         Length:921        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   DATES.8            DATES.9            DATES.10        \n Length:921         Length:921         Length:921        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n\n\n\nI entered a photo into chat gpt and that photo has values so it used that to recreate the graphs\n###How can I recreate this exact graph in R.\n\ndata &lt;- data.frame(\n  category = c(\"Winners\", \"Runners-up\", \"Third place\", \"Fourth place\", \"Fifth place and below\"),\n  count = c(4, 4, 3, 1, 11),\n  percentage = c(17.4, 17.4, 13.0, 4.3, 47.8)\n)\n\n# Define starting and ending x/y positions for connecting paths\ndata &lt;- data %&gt;%\n  mutate(x_start = 1, x_end = 2.5, \n         y_end = rev(seq(1, nrow(data) * 2, by = 2)), \n         y_start = rep(5, nrow(data)))  # Single starting point\n\n# Base plot with circles representing the first-impression roses\nggplot() +\n  geom_circle(aes(x0 = 1, y0 = 5, r = 1.5), fill = \"pink\", alpha = 0.5) +\n  geom_text(aes(x = 1, y = 5, label = \"23 first-impression roses\"), size = 5, fontface = \"bold\") +\n\n  # Connecting paths\n  geom_segment(data = data, aes(x = x_start, xend = x_end, y = y_start, yend = y_end),\n               color = \"pink\", size = 2, alpha = 0.7) +\n\n  # Rectangles for ranking outcomes\n  geom_rect(data = data, aes(xmin = x_end, xmax = x_end + 0.5, ymin = y_end - 0.5, ymax = y_end + 0.5), \n            fill = c(\"darkred\", \"pink\", \"lightpink\", \"lightgray\", \"gray\"), color = \"black\") +\n\n  # Text labels inside rectangles\n  geom_text(data = data, aes(x = x_end + 0.25, y = y_end, label = count), color = \"white\", fontface = \"bold\", size = 5) +\n\n  # Text labels for category names\n  geom_text(data = data, aes(x = x_end + 1, y = y_end, label = paste0(category, \"\\n\", percentage, \"%\")), \n            hjust = 0, size = 3, fontface = \"bold\") +\n\n  # Theme adjustments\n  theme_void() +\n  ggtitle(\"Where first-impression roses lead\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\nI wanted the modify the design a bit to match the orignal more so I asked chat gpt this\n###The design is good but I want there to be 23 roses inside the “first impression roses” bubble and I want the lines connecting to the counts to match the color of the count box. ignore the dataset, modify the code previously given.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(ggforce)\nlibrary(dplyr)\nlibrary(ggimage)\n\n# Define dataset for categories and colors\ndata &lt;- data.frame(\n  category = c(\"Winners\", \"Runners-up\", \"Third place\", \"Fourth place\", \"Fifth place and below\"),\n  count = c(4, 4, 3, 1, 11),\n  percentage = c(17.4, 17.4, 13.0, 4.3, 47.8),\n  color = c(\"darkred\", \"pink\", \"lightpink\", \"lightgray\", \"gray\")\n)\n\n# Define starting and ending x/y positions for connecting paths\ndata &lt;- data %&gt;%\n  mutate(x_start = 1, x_end = 2.5, \n         y_end = rev(seq(1, nrow(data) * 2, by = 2)), \n         y_start = rep(5, nrow(data)))\n\n# Generate 23 random (x, y) positions inside the circle\nset.seed(123)  # For reproducibility\nn_roses &lt;- 23\nangles &lt;- runif(n_roses, 0, 2 * pi)  # Random angles\nradii &lt;- sqrt(runif(n_roses, 0, 1)) * 1.1 # Random radii to keep roses inside\n\n# Convert polar to Cartesian coordinates\nroses &lt;- data.frame(\n  x = 1 + radii * cos(angles),\n  y = 5 + radii * sin(angles)\n)\n\nrose_image &lt;- \"/Users/alexisgonzalez/Desktop/MADA/alexisgonzalez-MADA-portfolio/presentation-exercise/images/Rose.png\"   \n\n# Create the plot\nggplot() +\n  \n  # Bubble with scattered roses\n  geom_circle(aes(x0 = 1, y0 = 5, r = 1.5), fill = \"white\", color = \"black\") +\n  geom_image(data = roses, aes(x = x, y = y, image = rose_image), size = 0.06) +\n  geom_text(aes(x = 1, y = 7, label = \"23 first-impression roses\"), size = 5, fontface = \"bold\") +\n  \n  # Connecting paths with matching colors\n  geom_segment(data = data, aes(x = x_start, xend = x_end, y = y_start, yend = y_end, color = color),\n               size = 2, alpha = 0.8, lineend = \"round\") +\n  \n  # Rectangles for ranking outcomes\n  geom_rect(data = data, aes(xmin = x_end, xmax = x_end + 0.5, ymin = y_end - 0.5, ymax = y_end + 0.5, fill = color),\n            color = \"black\") +\n\n  # Text labels inside rectangles\n  geom_text(data = data, aes(x = x_end + 0.25, y = y_end, label = count), color = \"white\", fontface = \"bold\", size = 5) +\n\n  # Text labels for category names\n  geom_text(data = data, aes(x = x_end + 1, y = y_end, label = paste0(category, \"\\n\", percentage, \"%\")), \n            hjust = 0, size = 5, fontface = \"bold\") +\n\n  # Theme adjustments\n  scale_fill_identity() +\n  scale_color_identity() +\n  theme_void() +\n  ggtitle(\"Where first-impression roses lead\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\nNow I will create a data table that shows who are in the categories shown in the graph above, what season they’re in and how many roses they won all season.\n#####First I want to filter out all contestants that did not recieve a first impression rose\n\nfirst_impression_contestants &lt;- bach %&gt;%\n  filter(ELIMINATION.1 == \"R1\")\n\n\nsummary(first_impression_contestants)\n\n     SHOW              SEASON           CONTESTANT        ELIMINATION.1     \n Length:45          Length:45          Length:45          Length:45         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION.2      ELIMINATION.3      ELIMINATION.4      ELIMINATION.5     \n Length:45          Length:45          Length:45          Length:45         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION.6      ELIMINATION.7      ELIMINATION.8      ELIMINATION.9     \n Length:45          Length:45          Length:45          Length:45         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION.10       DATES.1            DATES.2            DATES.3         \n Length:45          Length:45          Length:45          Length:45         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   DATES.4            DATES.5            DATES.6            DATES.7         \n Length:45          Length:45          Length:45          Length:45         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   DATES.8            DATES.9            DATES.10        \n Length:45          Length:45          Length:45         \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n\nstr(first_impression_contestants)\n\n'data.frame':   45 obs. of  23 variables:\n $ SHOW          : chr  \"Bachelorette\" \"Bachelorette\" \"Bachelorette\" \"Bachelorette\" ...\n $ SEASON        : chr  \"13\" \"12\" \"11\" \"10\" ...\n $ CONTESTANT    : chr  \"13_BRYAN_A\" \"12_JORDAN_R\" \"11_SHAWN_B\" \"10_NICK_V\" ...\n $ ELIMINATION.1 : chr  \"R1\" \"R1\" \"R1\" \"R1\" ...\n $ ELIMINATION.2 : chr  \"\" \"\" \"\" \"\" ...\n $ ELIMINATION.3 : chr  \"\" \"\" \"R\" \"R\" ...\n $ ELIMINATION.4 : chr  \"R\" \"R\" \"\" \"\" ...\n $ ELIMINATION.5 : chr  \"R\" \"R\" \"R\" \"\" ...\n $ ELIMINATION.6 : chr  \"\" \"\" \"\" \"R\" ...\n $ ELIMINATION.7 : chr  \"R\" \"\" \"\" \"R\" ...\n $ ELIMINATION.8 : chr  \"\" \"\" \"\" \"\" ...\n $ ELIMINATION.9 : chr  \"\" \"\" \"W\" \"\" ...\n $ ELIMINATION.10: chr  \"W\" \"W\" \"\" \"EU\" ...\n $ DATES.1       : chr  \"\" \"\" \"\" \"\" ...\n $ DATES.2       : chr  \"\" \"D6\" \"\" \"\" ...\n $ DATES.3       : chr  \"D6\" \"D12\" \"D6\" \"D1\" ...\n $ DATES.4       : chr  \"D13\" \"D11\" \"D8\" \"D6\" ...\n $ DATES.5       : chr  \"D1\" \"D1\" \"D1\" \"D9\" ...\n $ DATES.6       : chr  \"D7\" \"D5\" \"\" \"D1\" ...\n $ DATES.7       : chr  \"D1\" \"D1\" \"D3\" \"D4\" ...\n $ DATES.8       : chr  \"D1\" \"D1\" \"D1\" \"D1\" ...\n $ DATES.9       : chr  \"D1\" \"D1\" \"D1\" \"D1\" ...\n $ DATES.10      : chr  \"D1\" \"D1\" \"\" \"D1\" ...\n\n\n\nI noticed that the dataset was showing multiple people getting the first impression rose on a single season. Only 1 of those are given out per season so I will remove the entries that did not actaully recieve the first impression rose. I can figure this out with a simple google search.\n\n# Reset row names\nrow.names(first_impression_contestants) &lt;- NULL\nfirst_imp2 &lt;- first_impression_contestants[-c(5, 6, 7, 9, 10, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 44, 16, 17), ]\n\n\nfirst_imp2\n\n           SHOW SEASON     CONTESTANT ELIMINATION.1 ELIMINATION.2 ELIMINATION.3\n1  Bachelorette     13     13_BRYAN_A            R1                            \n2  Bachelorette     12    12_JORDAN_R            R1                            \n3  Bachelorette     11     11_SHAWN_B            R1                           R\n4  Bachelorette     10      10_NICK_V            R1                           R\n8  Bachelorette     09       09_BEN_S            R1             R              \n11 Bachelorette     08      08_DOUG_C            R1                            \n12 Bachelorette     07      07_RYAN_P            R1                           R\n13 Bachelorette     06    06_ROBERT_M            R1                           R\n14 Bachelorette     05     05_DAVID_G            R1                            \n15 Bachelorette     04     04_JESSE_C            R1                            \n18 Bachelorette     03       03_KEITH            R1                           E\n19     Bachelor     21   21_VANESSA_G            R1                           R\n20     Bachelor     20    20_OLIVIA_C            R1             R              \n21     Bachelor     19     19_BRITT_N            R1                            \n22     Bachelor     18  18_SHARLEEN_J            R1                            \n26     Bachelor     17    17_TIERRA_L            R1                            \n35     Bachelor     16    16_LINDZI_C            R1                           R\n36     Bachelor     15    15_ASHLEY_S            R1                           R\n37     Bachelor     14    14_TENLEY_M            R1                            \n38     Bachelor     13     13_NIKKI_K            R1                            \n39     Bachelor     12    12_AMANDA_R            R1                            \n40     Bachelor     11     11_JENNI_C            R1                            \n41     Bachelor     10 10_STEPHANIE_T            R1             R              \n42     Bachelor     09      09_LISA_B            R1             R              \n43     Bachelor     07    07_Sarah W.            R1             R              \n45     Bachelor     05     05_TRISH_S            R1                            \n   ELIMINATION.4 ELIMINATION.5 ELIMINATION.6 ELIMINATION.7 ELIMINATION.8\n1              R             R                           R              \n2              R             R                                          \n3                            R                                          \n4                                          R             R              \n8                           ED                                          \n11             R                                        ED              \n12                                         R            ED              \n13                                                                      \n14             E                                                        \n15             R                                                       W\n18                                                                      \n19                                                                      \n20                           R            ED                            \n21                           R             R             E              \n22             R                           R            EQ              \n26             R             R                          EU              \n35                                         R                            \n36                          ED                                          \n37             R                                                       E\n38            ED                                                        \n39             R                                         E              \n40             R                                                       E\n41             E                                                        \n42                                         E                            \n43                                         E                            \n45                           E                                          \n   ELIMINATION.9 ELIMINATION.10 DATES.1 DATES.2 DATES.3 DATES.4 DATES.5 DATES.6\n1                             W                      D6     D13      D1      D7\n2                             W              D6     D12     D11      D1      D5\n3              W                                     D6      D8      D1        \n4                            EU                      D1      D6      D9      D1\n8                                           D14     D10     D10      D2        \n11                                                  D12      D1      D8      D6\n12                                                  D10     D10      D8      D6\n13                            W                      D1      D7      D6      D4\n14                                           D7             D10                \n15                                           D7     D10      D1      D4      D1\n18                                           D1      D6                        \n19                            W             D12      D1     D13     D10      D6\n20                                           D6     D12     D12      D9      D2\n21                                          D11     D12      D6      D1      D6\n22                                                  D10      D1      D9      D6\n26                                          D13     D12      D8      D2      D7\n35                            E                      D1      D8      D9      D6\n36                                                   D1      D9      D2        \n37                                                   D8      D6      D1      D1\n38                                           D8      D8      D2                \n39                                           D8     D10      D1      D4      D1\n40                                           D7      D6      D1      D4      D1\n41                                           D1      D5      D4                \n42                                           D1      D6      D4      D1      D1\n43                                   D9      D1      D6      D5      D1      D1\n45                                           D1      D2      D3      D1        \n   DATES.7 DATES.8 DATES.9 DATES.10\n1       D1      D1      D1       D1\n2       D1      D1      D1       D1\n3       D3      D1      D1         \n4       D4      D1      D1       D1\n8                                  \n11      D3                         \n12      D1                         \n13      D1      D1      D1       D1\n14                                 \n15      D1      D1                 \n18                                 \n19      D1      D1      D1       D1\n20                                 \n21      D3                         \n22      D1                         \n26                                 \n35      D1      D1      D1       D1\n36                                 \n37      D1      D1                 \n38                                 \n39      D1                         \n40      D1      D1                 \n41                                 \n42                                 \n43                                 \n45                                 \n\n\n\nlength(first_imp2$CONTESTANT)\n\n[1] 26\n\nlength(first_imp2$SEASON)\n\n[1] 26\n\nlength(first_imp2$SHOW)\n\n[1] 26\n\n\n####Now I will create the table, inserting a photo on the contestant\n\n# Create the data frame with contestant details\ncontestants_data &lt;- data.frame(\n  contestant = first_imp2$CONTESTANT,\n  season = first_imp2$SEASON,\n  show = first_imp2$SHOW,\n  category = c(\"Fifth place and Below\",\"Winners\",\"Fifth place and Below\",\"Fifth place and Below\",\"Winners\", \"Winners\", \"Runners-up\", \"Fifth place and Below\",\"Fourth place\",\"Fourth place\",\"Winners\",\"Fifth place and Below\",\"Winners\",\"Fifth place and Below\",\"Winners\",\"Fifth place and Below\",\"Fourth place\",\"Fourth place\",\"Fourth place\",\"Runners-up\",\"Third Place\",\"Fourth place\",\"Third Place\",\"Fifth place and Below\",\"Fifth place and Below\",\"Fifth place and Below\"),\n  roses_received = c(4, 3, 3, 3, 2, 2, 3, 2, 1, 2, 2, 1, 2, 3, 2, 3, 2, 1, 2, 3, 3, 2, 2, 2, 2, 1),\n  relationship_status = c(\"Single\", \"Single\", \"Single\", \"Single\", \"Single\", \"Single\", \"Single\", \"Single\", \"Married\", \"Single\", \"Single\", \"Single\", \"Single\", \"Single\", \"Single\", \"Married\", \"Single\", \"Single\", \"Single\", \"Single\", \"Single\", \"Married\", \"Single\", \"Single\", \"Single\", \"Single\"),\n  image_url = c(\"himages.png\",  # Example image URL for \"Single\"\n                \"himages.png\",  # Example image URL for \"In a Relationship\"\n                \"himages.png\",  # Example image URL for \"Single\"\n                \"himages.png\",  # Example image URL for \"In a Relationship\"\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\",\n                \"himages.png\") \n)\ncontestants_data$image_url &lt;- \"himages.png\"  \n# Replace relationship_status with image HTML\ncontestants_data$relationship_status &lt;- paste0('&lt;img src=\"',contestants_data$image_url, '\" width=\"50\" height=\"50\"&gt;')\n\n# Load necessary libraries\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(dplyr)\n\n# Create the table using kableExtra\n# Create the table using kableExtra\ncontestants_data %&gt;%\n  kable(\"html\", escape = FALSE) %&gt;%  # Create HTML table\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE, position = \"center\") %&gt;%  # Stripe and hover effects\n  column_spec(1, bold = TRUE, color = \"white\", background = \"darkpink\") %&gt;%  # Style 'contestant' column\n  column_spec(2, bold = TRUE) %&gt;%  # Bold 'season' column\n  column_spec(3, background = \"lightpink\") %&gt;%  # Style 'show' column\n  column_spec(4, color = \"blue\") %&gt;%  # Style 'category' column\n  column_spec(5, background = \"lightpink\") %&gt;%  # Style 'roses_received' column\n  column_spec(6, background = \"lightpink\") %&gt;%  # Style 'relationship_status' column\n  add_header_above(c(\" \", \"Contestants Info\" = 6))  # Add header and table title\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContestants Info\n\n\n\ncontestant\nseason\nshow\ncategory\nroses_received\nrelationship_status\nimage_url\n\n\n\n\n13_BRYAN_A\n13\nBachelorette\nFifth place and Below\n4\n\nhimages.png\n\n\n12_JORDAN_R\n12\nBachelorette\nWinners\n3\n\nhimages.png\n\n\n11_SHAWN_B\n11\nBachelorette\nFifth place and Below\n3\n\nhimages.png\n\n\n10_NICK_V\n10\nBachelorette\nFifth place and Below\n3\n\nhimages.png\n\n\n09_BEN_S\n09\nBachelorette\nWinners\n2\n\nhimages.png\n\n\n08_DOUG_C\n08\nBachelorette\nWinners\n2\n\nhimages.png\n\n\n07_RYAN_P\n07\nBachelorette\nRunners-up\n3\n\nhimages.png\n\n\n06_ROBERT_M\n06\nBachelorette\nFifth place and Below\n2\n\nhimages.png\n\n\n05_DAVID_G\n05\nBachelorette\nFourth place\n1\n\nhimages.png\n\n\n04_JESSE_C\n04\nBachelorette\nFourth place\n2\n\nhimages.png\n\n\n03_KEITH\n03\nBachelorette\nWinners\n2\n\nhimages.png\n\n\n21_VANESSA_G\n21\nBachelor\nFifth place and Below\n1\n\nhimages.png\n\n\n20_OLIVIA_C\n20\nBachelor\nWinners\n2\n\nhimages.png\n\n\n19_BRITT_N\n19\nBachelor\nFifth place and Below\n3\n\nhimages.png\n\n\n18_SHARLEEN_J\n18\nBachelor\nWinners\n2\n\nhimages.png\n\n\n17_TIERRA_L\n17\nBachelor\nFifth place and Below\n3\n\nhimages.png\n\n\n16_LINDZI_C\n16\nBachelor\nFourth place\n2\n\nhimages.png\n\n\n15_ASHLEY_S\n15\nBachelor\nFourth place\n1\n\nhimages.png\n\n\n14_TENLEY_M\n14\nBachelor\nFourth place\n2\n\nhimages.png\n\n\n13_NIKKI_K\n13\nBachelor\nRunners-up\n3\n\nhimages.png\n\n\n12_AMANDA_R\n12\nBachelor\nThird Place\n3\n\nhimages.png\n\n\n11_JENNI_C\n11\nBachelor\nFourth place\n2\n\nhimages.png\n\n\n10_STEPHANIE_T\n10\nBachelor\nThird Place\n2\n\nhimages.png\n\n\n09_LISA_B\n09\nBachelor\nFifth place and Below\n2\n\nhimages.png\n\n\n07_Sarah W.\n07\nBachelor\nFifth place and Below\n2\n\nhimages.png\n\n\n05_TRISH_S\n05\nBachelor\nFifth place and Below\n1\n\nhimages.png"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-excercise.html",
    "href": "cdcdata-exercise/cdcdata-excercise.html",
    "title": "Moduel 5 Exercise",
    "section": "",
    "text": "CDC Data exploration\n\nThe data set being used for this exercise is the Prevalence of Disability Statues and Types by Demographic Groups (2021) from the Disability and Health Data System (DHDS). DHDS collects state-level data on adults with disabilities. Disabilities are categorized by 6 disability types: cognitive, hearing, mobility, vision, self care (difficulty dressing or bathing), and independent living. This data is collected through the Behavioral Risk Factor Surveillance System using telephone interviews. Participants must be non-institutionalized and 18 years or older. The data can be found here https://data.cdc.gov/Disability-Health/DHDS-Prevalence-of-Disability-Status-and-Types-by-/qjg3-6acf/about_data .\n\n\n\n\nNow I will load in the dataset\n\ncdcdata &lt;- read_csv(\"DHDS_-_Prevalence_of_Disability_Status_and_Types_by_Demographic_Groups__2021_20250204.csv\")\n\nRows: 7168 Columns: 30\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): LocationAbbr, LocationDesc, DataSource, Category, Indicator, Respo...\ndbl  (8): Year, Data_Value, Data_Value_Alt, Low_Confidence_Limit, High_Confi...\nlgl  (4): StratificationCategory2, Stratification2, StratificationCategoryID...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nstr(cdcdata)\n\nspc_tbl_ [7,168 × 30] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Year                      : num [1:7168] 2021 2021 2021 2021 2021 ...\n $ LocationAbbr              : chr [1:7168] \"GU\" \"HHS8\" \"GU\" \"GU\" ...\n $ LocationDesc              : chr [1:7168] \"Guam\" \"HHS Region 8\" \"Guam\" \"Guam\" ...\n $ DataSource                : chr [1:7168] \"BRFSS\" \"BRFSS\" \"BRFSS\" \"BRFSS\" ...\n $ Category                  : chr [1:7168] \"Disability Estimates\" \"Disability Estimates\" \"Disability Estimates\" \"Disability Estimates\" ...\n $ Indicator                 : chr [1:7168] \"Disability status and types among adults 18 years of age or older by race/ethnicity\" \"Disability status and types among adults 18 years of age or older by age group\" \"Disability status and types among adults 18 years of age or older by age group\" \"Disability status and types among adults 18 years of age or older by age group\" ...\n $ Response                  : chr [1:7168] \"Black, non-Hispanic\" \"45-64\" \"45-64\" \"18-44\" ...\n $ Data_Value_Unit           : chr [1:7168] \"%\" \"%\" \"%\" \"%\" ...\n $ Data_Value_Type           : chr [1:7168] \"Age-adjusted Prevalence\" \"Prevalence\" \"Prevalence\" \"Prevalence\" ...\n $ Data_Value                : num [1:7168] NA 3.5 56.2 19.7 65.1 27.8 57.8 15.4 NA 9.4 ...\n $ Data_Value_Alt            : num [1:7168] NA 3.5 56.2 19.7 65.1 27.8 57.8 15.4 NA 9.4 ...\n $ Data_Value_Footnote_Symbol: chr [1:7168] \"*\" NA NA NA ...\n $ Data_Value_Footnote       : chr [1:7168] \"Data suppressed\" NA NA NA ...\n $ Low_Confidence_Limit      : num [1:7168] NA 3.1 47.2 15.3 62.5 25.6 43.3 14.1 NA 8.4 ...\n $ High_Confidence_Limit     : num [1:7168] NA 3.9 64.7 25.1 67.7 30.1 70.9 16.7 NA 10.5 ...\n $ Number                    : num [1:7168] NA 516 393 105 1657 ...\n $ WeightedNumber            : num [1:7168] NA 95963 17420 11422 1092182 ...\n $ StratificationCategory1   : chr [1:7168] \"Disability Status\" \"Disability Type\" \"Disability Status\" \"Disability Status\" ...\n $ Stratification1           : chr [1:7168] \"Any Disability\" \"Self-care Disability\" \"No Disability\" \"Any Disability\" ...\n $ StratificationCategory2   : logi [1:7168] NA NA NA NA NA NA ...\n $ Stratification2           : logi [1:7168] NA NA NA NA NA NA ...\n $ CategoryID                : chr [1:7168] \"DISEST\" \"DISEST\" \"DISEST\" \"DISEST\" ...\n $ IndicatorID               : chr [1:7168] \"RACEIND\" \"AGEIND\" \"AGEIND\" \"AGEIND\" ...\n $ LocationID                : num [1:7168] 66 87 66 66 22 16 78 59 87 33 ...\n $ ResponseID                : chr [1:7168] \"RACE02\" \"AGE02\" \"AGE02\" \"AGE01\" ...\n $ DataValueTypeID           : chr [1:7168] \"AGEADJPREV\" \"PREV\" \"PREV\" \"PREV\" ...\n $ StratificationCategoryID1 : chr [1:7168] \"DISSTAT\" \"DISTYPE\" \"DISSTAT\" \"DISSTAT\" ...\n $ StratificationID1         : chr [1:7168] \"DISABL\" \"SELFDIS\" \"NODIS\" \"DISABL\" ...\n $ StratificationCategoryID2 : logi [1:7168] NA NA NA NA NA NA ...\n $ StratificationID2         : logi [1:7168] NA NA NA NA NA NA ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Year = col_double(),\n  ..   LocationAbbr = col_character(),\n  ..   LocationDesc = col_character(),\n  ..   DataSource = col_character(),\n  ..   Category = col_character(),\n  ..   Indicator = col_character(),\n  ..   Response = col_character(),\n  ..   Data_Value_Unit = col_character(),\n  ..   Data_Value_Type = col_character(),\n  ..   Data_Value = col_double(),\n  ..   Data_Value_Alt = col_double(),\n  ..   Data_Value_Footnote_Symbol = col_character(),\n  ..   Data_Value_Footnote = col_character(),\n  ..   Low_Confidence_Limit = col_double(),\n  ..   High_Confidence_Limit = col_double(),\n  ..   Number = col_double(),\n  ..   WeightedNumber = col_double(),\n  ..   StratificationCategory1 = col_character(),\n  ..   Stratification1 = col_character(),\n  ..   StratificationCategory2 = col_logical(),\n  ..   Stratification2 = col_logical(),\n  ..   CategoryID = col_character(),\n  ..   IndicatorID = col_character(),\n  ..   LocationID = col_double(),\n  ..   ResponseID = col_character(),\n  ..   DataValueTypeID = col_character(),\n  ..   StratificationCategoryID1 = col_character(),\n  ..   StratificationID1 = col_character(),\n  ..   StratificationCategoryID2 = col_logical(),\n  ..   StratificationID2 = col_logical()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(cdcdata)\n\n      Year      LocationAbbr       LocationDesc        DataSource       \n Min.   :2021   Length:7168        Length:7168        Length:7168       \n 1st Qu.:2021   Class :character   Class :character   Class :character  \n Median :2021   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2021                                                           \n 3rd Qu.:2021                                                           \n Max.   :2021                                                           \n                                                                        \n   Category          Indicator           Response         Data_Value_Unit   \n Length:7168        Length:7168        Length:7168        Length:7168       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n Data_Value_Type      Data_Value    Data_Value_Alt  Data_Value_Footnote_Symbol\n Length:7168        Min.   : 0.40   Min.   : 0.40   Length:7168               \n Class :character   1st Qu.: 5.80   1st Qu.: 5.80   Class :character          \n Mode  :character   Median :11.30   Median :11.30   Mode  :character          \n                    Mean   :20.79   Mean   :20.79                             \n                    3rd Qu.:25.60   3rd Qu.:25.60                             \n                    Max.   :91.10   Max.   :91.10                             \n                    NA's   :1623    NA's   :1623                              \n Data_Value_Footnote Low_Confidence_Limit High_Confidence_Limit\n Length:7168         Min.   : 0.20        Min.   : 0.80        \n Class :character    1st Qu.: 4.60        1st Qu.: 7.30        \n Mode  :character    Median : 9.30        Median :13.70        \n                     Mean   :18.33        Mean   :23.62        \n                     3rd Qu.:22.40        3rd Qu.:29.10        \n                     Max.   :87.40        Max.   :96.10        \n                     NA's   :1623         NA's   :1623         \n     Number       WeightedNumber      StratificationCategory1\n Min.   :     2   Min.   :      450   Length:7168            \n 1st Qu.:    99   1st Qu.:    32514   Class :character       \n Median :   276   Median :   117426   Mode  :character       \n Mean   :  1367   Mean   :   744895                          \n 3rd Qu.:   852   3rd Qu.:   421110                          \n Max.   :257800   Max.   :150555661                          \n NA's   :1623     NA's   :1623                               \n Stratification1    StratificationCategory2 Stratification2  CategoryID       \n Length:7168        Mode:logical            Mode:logical    Length:7168       \n Class :character   NA's:7168               NA's:7168       Class :character  \n Mode  :character                                           Mode  :character  \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n IndicatorID          LocationID     ResponseID        DataValueTypeID   \n Length:7168        Min.   : 1.00   Length:7168        Length:7168       \n Class :character   1st Qu.:20.75   Class :character   Class :character  \n Mode  :character   Median :36.50   Mode  :character   Mode  :character  \n                    Mean   :40.39                                        \n                    3rd Qu.:54.25                                        \n                    Max.   :89.00                                        \n                                                                         \n StratificationCategoryID1 StratificationID1  StratificationCategoryID2\n Length:7168               Length:7168        Mode:logical             \n Class :character          Class :character   NA's:7168                \n Mode  :character          Mode  :character                            \n                                                                       \n                                                                       \n                                                                       \n                                                                       \n StratificationID2\n Mode:logical     \n NA's:7168        \n                  \n                  \n                  \n                  \n                  \n\n\n\nWe can use this command to see how the data is groups\n\nunique(cdcdata$Response)\n\n [1] \"Black, non-Hispanic\"                                    \n [2] \"45-64\"                                                  \n [3] \"18-44\"                                                  \n [4] \"Female\"                                                 \n [5] \"65+\"                                                    \n [6] \"Other / Multirace, non-Hispanic\"                        \n [7] \"Non-Veteran\"                                            \n [8] \"Asian, non-Hispanic\"                                    \n [9] \"American Indian or Alaska Native, non-Hispanic\"         \n[10] \"Male\"                                                   \n[11] \"Veteran\"                                                \n[12] \"Hispanic\"                                               \n[13] \"Native Hawaiian or Other Pacific Islander, non-Hispanic\"\n[14] \"White, non-Hispanic\"                                    \n\n\nWe see here that response contains the answers to the demographic questions. When we do exploratory analysis we have to be sure to specify what response we want.\n\nunique(cdcdata$Indicator)\n\n[1] \"Disability status and types among adults 18 years of age or older by race/ethnicity\"\n[2] \"Disability status and types among adults 18 years of age or older by age group\"     \n[3] \"Disability status and types among adults 18 years of age or older by sex\"           \n[4] \"Disability status and types among adults 18 years of age or older by veteran status\"\n\n\n\nunique(cdcdata$LocationDesc)\n\n [1] \"Guam\"                            \"HHS Region 8\"                   \n [3] \"Louisiana\"                       \"Idaho\"                          \n [5] \"U.S. Virgin Islands\"             \"United States, DC & Territories\"\n [7] \"New Hampshire\"                   \"Rhode Island\"                   \n [9] \"Nebraska\"                        \"Nevada\"                         \n[11] \"Connecticut\"                     \"Arkansas\"                       \n[13] \"HHS Region 1\"                    \"Texas\"                          \n[15] \"Oklahoma\"                        \"HHS Region 4\"                   \n[17] \"HHS Region 5\"                    \"Missouri\"                       \n[19] \"Washington\"                      \"Massachusetts\"                  \n[21] \"HHS Region 10\"                   \"Minnesota\"                      \n[23] \"Georgia\"                         \"California\"                     \n[25] \"District of Columbia\"            \"Delaware\"                       \n[27] \"Alabama\"                         \"Ohio\"                           \n[29] \"Puerto Rico\"                     \"North Carolina\"                 \n[31] \"HHS Region 2\"                    \"Iowa\"                           \n[33] \"Montana\"                         \"Kansas\"                         \n[35] \"West Virginia\"                   \"New York\"                       \n[37] \"Tennessee\"                       \"Michigan\"                       \n[39] \"Maine\"                           \"Wisconsin\"                      \n[41] \"Hawaii\"                          \"Mississippi\"                    \n[43] \"Utah\"                            \"New Mexico\"                     \n[45] \"HHS Region 6\"                    \"Arizona\"                        \n[47] \"HHS Region 7\"                    \"Colorado\"                       \n[49] \"New Jersey\"                      \"HHS Region 9\"                   \n[51] \"Virginia\"                        \"HHS Region 3\"                   \n[53] \"Illinois\"                        \"Pennsylvania\"                   \n[55] \"South Dakota\"                    \"North Dakota\"                   \n[57] \"Kentucky\"                        \"Maryland\"                       \n[59] \"Wyoming\"                         \"Vermont\"                        \n[61] \"Indiana\"                         \"Oregon\"                         \n[63] \"South Carolina\"                  \"Alaska\"                         \n\n\n\n\n\nSome variables have no data for any entry so I will remove those variable and keep the ones I want\n\ncdcdata2 &lt;- cdcdata %&gt;%\n  select(LocationDesc, Category,Indicator,Response,StratificationCategory1,Stratification1, Data_Value)\n\n\n\nI also want to keep the data to the 50 states so I will remove territiories (except for DC) and HHS Regions.\n\ncdcdata_final &lt;- cdcdata2 %&gt;%\n  filter(!(LocationDesc %in% c(\"Guam\", \"U.S. Virgin Islands\",\"HHS Region 1\",\"HHS Region 2\",\"HHS Region 3\",\"HHS Region 4\",\"HHS Region 5\",\"HHS Region 6\",\"HHS Region 7\",\"HHS Region 8\", \"HHS Region 9\",\"HHS Region 10\",\"Puerto Rico\", \"United States, DC & Territories\")))\n\n\n\nChecking to make sure it worked\n\nunique(cdcdata_final$LocationDesc)\n\n [1] \"Louisiana\"            \"Idaho\"                \"New Hampshire\"       \n [4] \"Rhode Island\"         \"Nebraska\"             \"Nevada\"              \n [7] \"Connecticut\"          \"Arkansas\"             \"Texas\"               \n[10] \"Oklahoma\"             \"Missouri\"             \"Washington\"          \n[13] \"Massachusetts\"        \"Minnesota\"            \"Georgia\"             \n[16] \"California\"           \"District of Columbia\" \"Delaware\"            \n[19] \"Alabama\"              \"Ohio\"                 \"North Carolina\"      \n[22] \"Iowa\"                 \"Montana\"              \"Kansas\"              \n[25] \"West Virginia\"        \"New York\"             \"Tennessee\"           \n[28] \"Michigan\"             \"Maine\"                \"Wisconsin\"           \n[31] \"Hawaii\"               \"Mississippi\"          \"Utah\"                \n[34] \"New Mexico\"           \"Arizona\"              \"Colorado\"            \n[37] \"New Jersey\"           \"Virginia\"             \"Illinois\"            \n[40] \"Pennsylvania\"         \"South Dakota\"         \"North Dakota\"        \n[43] \"Kentucky\"             \"Maryland\"             \"Wyoming\"             \n[46] \"Vermont\"              \"Indiana\"              \"Oregon\"              \n[49] \"South Carolina\"       \"Alaska\"              \n\n\n\n\nNow that the data is cleaned and condensed we can do some data exploration\n\n\nThere are lots of variables to look at so lets start with just looking at a histogram of disability by age group.Starting with those that are 18-44\n\neighteento44 &lt;- cdcdata_final %&gt;%\n  filter(Response == \"18-44\", Indicator == \"Disability status and types among adults 18 years of age or older by age group\")\n\n\nggplot(eighteento44, aes(x = Stratification1 , y= Data_Value, color = Stratification1)) +\n  geom_boxplot() +\n  labs(title = \"Disabilities in ages 18-44 in 2021\",\n       x = \"Disability Type\",\n       y = \"Prevelance\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: Removed 10 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nNow we will repeat this for the other age groups\n\nagegroup2 &lt;- cdcdata_final %&gt;%\n  filter(Response == \"45-64\", Indicator == \"Disability status and types among adults 18 years of age or older by age group\")\n\n\nggplot(agegroup2, aes(x = Stratification1 , y= Data_Value, color = Stratification1)) +\n  geom_boxplot() +\n  labs(title = \"Disabilities in ages 45-64 in 2021\",\n       x = \"Disability Type\",\n       y = \"Prevelance\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nagegroup2 &lt;- cdcdata_final %&gt;%\n  filter(Response == \"65+\", Indicator == \"Disability status and types among adults 18 years of age or older by age group\")\n\n\nggplot(agegroup2, aes(x = Stratification1 , y= Data_Value, color = Stratification1)) +\n  geom_boxplot() +\n  labs(title = \"Disabilities in age 65+ in 2021\",\n       x = \"Disability Type\",\n       y = \"Prevelance\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nThese 3 boxplots tell us how prevalence of disabilities changes as age increases. If we look at just the “Any Disability” category we see the prevalence noticably increase as age increases. We also see that a majority of those sampled have no disability.\n#####Now we will see if there is any correlation to age and disability, for this we will make a variable that combines all the disability entries minus those that selected no disability. and a new variable for age\n\ncdcdata_final &lt;- cdcdata_final %&gt;%\n  mutate(disability_yes = case_when(\n    Stratification1 %in% c(\"Any Disability\", \"Cognitive Disability\", \"Hearing Disability\", \"Independent Living Disability\", \"Mobility Disability\", \"Self-care Disability\", \"Vision Disability\") ~ \"Yes\",\n    Stratification1 == \"No Disability\" ~ \"No\",  \n    TRUE ~ NA_character_  \n  ))\n\n\ncdcdata_final &lt;- cdcdata_final %&gt;%\n  mutate(Ages = case_when(\n    Response == \"18-44\" ~ \"18-44\",\n    Response == \"45-65\" ~ \"45-65\",\n    Response == \"65+\" ~ \"65+\",\n    TRUE ~ NA_character_  \n  ))\n\n\n# Assuming `prevalence` is the outcome variable, and `ages` and `disability_yes` are predictors\nlm_model &lt;- lm(Data_Value ~ Ages + disability_yes, data = cdcdata_final)\n\n# View the summary of the model\nsummary(lm_model)\n\n\nCall:\nlm(formula = Data_Value ~ Ages + disability_yes, data = cdcdata_final)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.646  -7.649  -4.935   7.090  43.351 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        64.9818     1.1963  54.319  &lt; 2e-16 ***\nAges65+             5.1644     0.8021   6.439 2.09e-10 ***\ndisability_yesYes -54.9971     1.2060 -45.603  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.27 on 787 degrees of freedom\n  (4810 observations deleted due to missingness)\nMultiple R-squared:  0.7291,    Adjusted R-squared:  0.7284 \nF-statistic:  1059 on 2 and 787 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nNow lets look at age and disability\n\nI am going to approach this a little differently than before, I will first filter my Stratification 1 data to only contain “any disability” This will cut my entries down but it should give a good representation of disability prevelance in each race.\n\n# Filter data for 'any disability' (assuming 'disability' is the variable containing disability types)\ncdcdata_disability &lt;- cdcdata_final %&gt;%\n  filter(Stratification1 == \"Any Disability\")\n\n\n#filter data for races\ncdcdata_race &lt;- cdcdata_final %&gt;%\n  filter(Response %in% c(\"American Indian or Alaska Native, non-Hispanic\n\",\"Asian, non-Hispanic\",\"Black, non-Hispanic\",\"Hispanic\", \"Native Hawaiian or Other Pacific Islander, non-Hispanic\",\"Other / Multirace, non-Hispanic\",\"White, non-Hispanic\")) %&gt;%\n  filter(Stratification1== \"Any Disability\")\n\n\n# Create a bar plot for disability prevalence by race\nggplot(cdcdata_race, aes(x = Response, y = Data_Value, fill = Stratification1)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +  # Position bars side by side\n  labs(title = \"Prevalence of 'Any Disability' Across Racial Groups\", \n       x = \"Race\", \n       y = \"Disability Prevalence\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels\n\nWarning: Removed 77 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\n\nFrom this we gather that multiracial/other individuals had a higher prevalence of disability, lets see what disabilities.\n\n#filtering again to only include response = multirace\nmultirace &lt;- cdcdata_final %&gt;%\n  filter(Response == \"Other / Multirace, non-Hispanic\")\n\n\nggplot(multirace, aes(x = Stratification1 , y= Data_Value, color = Stratification1)) +\n  geom_boxplot() +\n  labs(title = \"Disabilities in those that are other/multirace in 2021\",\n       x = \"Disability Type\",\n       y = \"Prevelance\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: Removed 127 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nFrom these we see a higher prevalence of cognitive disability in those that are multirace/other\n\n\nNext I will do the same for each race\n\nblack &lt;- cdcdata_final %&gt;%\n  filter(Response == \"Black, non-Hispanic\")\nggplot(black, aes(x = Stratification1 , y= Data_Value, color = Stratification1)) +\n  geom_boxplot() +\n  labs(title = \"Disabilities in those that are other/multirace in 2021\",\n       x = \"Disability Type\",\n       y = \"Prevelance\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: Removed 121 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nasian &lt;- cdcdata_final %&gt;%\n  filter(Response == \"Asian, non-Hispanic\")\nggplot(asian, aes(x = Stratification1 , y= Data_Value, color = Stratification1)) +\n  geom_boxplot() +\n  labs(title = \"Disabilities in those that are other/multirace in 2021\",\n       x = \"Disability Type\",\n       y = \"Prevelance\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: Removed 309 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nnative &lt;- cdcdata_final %&gt;%\n  filter(Response == \"American Indian or Alaska Native, non-Hispanic\")\nggplot(native, aes(x = Stratification1 , y= Data_Value, color = Stratification1)) +\n  geom_boxplot() +\n  labs(title = \"Disabilities in those that are other/multirace in 2021\",\n       x = \"Disability Type\",\n       y = \"Prevelance\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: Removed 253 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nislander &lt;- cdcdata_final %&gt;%\n  filter(Response == \"Native Hawaiian or Other Pacific Islander, non-Hispanic\")\nggplot(islander, aes(x = Stratification1 , y= Data_Value, color = Stratification1)) +\n  geom_boxplot() +\n  labs(title = \"Disabilities in those that are other/multirace in 2021\",\n       x = \"Disability Type\",\n       y = \"Prevelance\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: Removed 390 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nwhite &lt;- cdcdata_final %&gt;%\n  filter(Response == \"White, non-Hispanic\")\nggplot(white, aes(x = Stratification1 , y= Data_Value, color = Stratification1)) +\n  geom_boxplot() +\n  labs(title = \"Disabilities in those that are other/multirace in 2021\",\n       x = \"Disability Type\",\n       y = \"Prevelance\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nThe last two demographics to see are sex and veteran status, I will be doing alot of what has previously been done for this section.\n\n# Filter data for 'any disability' (assuming 'disability' is the variable containing disability types)\ncdcdata_disability &lt;- cdcdata_final %&gt;%\n  filter(Stratification1 == \"Any Disability\")\n\n\n#filter data for genders\ncdcdata_sex &lt;- cdcdata_final %&gt;%\n  filter(Response %in% c(\"Female\", \"Male\" )) %&gt;%\n  filter(Stratification1== \"Any Disability\")\n\n\n# Create a bar plot for disability prevalence by gender\nggplot(cdcdata_sex, aes(x = Response, y = Data_Value, fill = Stratification1)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +  # Position bars side by side\n  labs(title = \"Prevalence of 'Any Disability' by sex\", \n       x = \"Sex\", \n       y = \"Disability Prevalence\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels\n\n\n\n\n\n\n\n\n\n#Disabilities in females\nfemale &lt;- cdcdata_final %&gt;%\n  filter(Response == \"Female\")\nggplot(female, aes(x = Stratification1 , y= Data_Value, color = Stratification1)) +\n  geom_boxplot() +\n  labs(title = \"Disabilities in those that are female\",\n       x = \"Disability Type\",\n       y = \"Prevelance\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n#Disabilities in males\nmale &lt;- cdcdata_final %&gt;%\n  filter(Response == \"Male\")\nggplot(male, aes(x = Stratification1 , y= Data_Value, color = Stratification1)) +\n  geom_boxplot() +\n  labs(title = \"Disabilities in those that are Male\",\n       x = \"Disability Type\",\n       y = \"Prevelance\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n# Filter data for 'any disability' (assuming 'disability' is the variable containing disability types)\ncdcdata_disability &lt;- cdcdata_final %&gt;%\n  filter(Stratification1 == \"Any Disability\")\n\n\n#filter data for vet status\ncdcdata_vet &lt;- cdcdata_final %&gt;%\n  filter(Response %in% c(\"Veteran\", \"Non-Veteran\" )) %&gt;%\n  filter(Stratification1== \"Any Disability\")\n\n\n# Create a bar plot for disability prevalence by gender\nggplot(cdcdata_vet, aes(x = Response, y = Data_Value, fill = Stratification1)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +  # Position bars side by side\n  labs(title = \"Prevalence of 'Any Disability' by Veteran Status\", \n       x = \"Status\", \n       y = \"Disability Prevalence\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels\n\n\n\n\n\n\n\n\n\n#Disabilities in veterans\nvet &lt;- cdcdata_final %&gt;%\n  filter(Response == \"Veteran\")\nggplot(vet, aes(x = Stratification1 , y= Data_Value, color = Stratification1)) +\n  geom_boxplot() +\n  labs(title = \"Disabilities in those that are Veterans\",\n       x = \"Disability Type\",\n       y = \"Prevelance\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: Removed 34 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nvet_no &lt;- cdcdata_final %&gt;%\n  filter(Response == \"Non-Veteran\")\nggplot(vet_no, aes(x = Stratification1 , y= Data_Value, color = Stratification1)) +\n  geom_boxplot() +\n  labs(title = \"Disabilities in those that are Not Veterans\",\n       x = \"Disability Type\",\n       y = \"Prevelance\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\nThis section contributed by Guozheng Yang\n\n\nThanks to Alexis for the effort!!!\nThe primary author analyzed the distribution of different types of disabilities among different demographic groups. The demographic characteristics of the study population are depict by age group, gender, race, and veteran status. First of all, I want to take a look at the frequency table of these demographic variables.\n\n# Frequency table of different \ntable(cdcdata_final$Response)\n\n\n                                                  18-44 \n                                                    400 \n                                                  45-64 \n                                                    400 \n                                                    65+ \n                                                    400 \n         American Indian or Alaska Native, non-Hispanic \n                                                    400 \n                                    Asian, non-Hispanic \n                                                    400 \n                                    Black, non-Hispanic \n                                                    400 \n                                                 Female \n                                                    400 \n                                               Hispanic \n                                                    400 \n                                                   Male \n                                                    400 \nNative Hawaiian or Other Pacific Islander, non-Hispanic \n                                                    400 \n                                            Non-Veteran \n                                                    400 \n                        Other / Multirace, non-Hispanic \n                                                    400 \n                                                Veteran \n                                                    400 \n                                    White, non-Hispanic \n                                                    400 \n\n\nAs shown, each category contains 400 records. Then I want to check how the data is structured by Disability Status and Disability Type within each category of Response.\n\n# Frequency table of StratificationCategory1 by Response\ncdcdata_final %&gt;%\n  count(Response, StratificationCategory1, Stratification1)\n\n# A tibble: 112 × 4\n   Response StratificationCategory1 Stratification1                   n\n   &lt;chr&gt;    &lt;chr&gt;                   &lt;chr&gt;                         &lt;int&gt;\n 1 18-44    Disability Status       Any Disability                   50\n 2 18-44    Disability Status       No Disability                    50\n 3 18-44    Disability Type         Cognitive Disability             50\n 4 18-44    Disability Type         Hearing Disability               50\n 5 18-44    Disability Type         Independent Living Disability    50\n 6 18-44    Disability Type         Mobility Disability              50\n 7 18-44    Disability Type         Self-care Disability             50\n 8 18-44    Disability Type         Vision Disability                50\n 9 45-64    Disability Status       Any Disability                   50\n10 45-64    Disability Status       No Disability                    50\n# ℹ 102 more rows\n\n\nSo, for each category in Response, there are 100 corresponding Disability Status with half being Any Disability and the other half being No Disability. Also, the rest 300 records for the Response category correspond to Disability Type, with 50 records for each of the disability types including Cognitive Disability, Hearing Disability, Independent Living Disability, Mobility Disability, Self-care Disability, and Vision Disability. Of note, this 50 records for each category in Response should denote the summary records for the 50 states in the US. Now I want to use Georgia data as an example and check this structure.\n\n# Frequency table of StratificationCategory1 by Response: Georgia\ncdcdata_final %&gt;%\n  filter(LocationDesc==\"Georgia\") %&gt;%\n  count(Response, StratificationCategory1, Stratification1)\n\n# A tibble: 112 × 4\n   Response StratificationCategory1 Stratification1                   n\n   &lt;chr&gt;    &lt;chr&gt;                   &lt;chr&gt;                         &lt;int&gt;\n 1 18-44    Disability Status       Any Disability                    1\n 2 18-44    Disability Status       No Disability                     1\n 3 18-44    Disability Type         Cognitive Disability              1\n 4 18-44    Disability Type         Hearing Disability                1\n 5 18-44    Disability Type         Independent Living Disability     1\n 6 18-44    Disability Type         Mobility Disability               1\n 7 18-44    Disability Type         Self-care Disability              1\n 8 18-44    Disability Type         Vision Disability                 1\n 9 45-64    Disability Status       Any Disability                    1\n10 45-64    Disability Status       No Disability                     1\n# ℹ 102 more rows\n\n\nIt confirms that each state will have a list of Response categories with corresponding StratificationCategory1 and Stratification1. As the primary author analyzed the distribution of Data_Value by Stratification1 within different Response categories, I will follow the same route.\nFirst of all, let’s create a new data frame to store the simulation data. At this first step, we can just fill in what’s known for sure and leave the numeric variable (i.e., Data_Value) which needs distribution parameters to be generated. Of note, the original dataset mixed the categories about age group, gender, race, and veteran status. I will simulate different datasets for each of these demographic factors.\n\n# Data frame for simulation data: age\nsim_age &lt;- data.frame(Response=rep(c(rep(\"18-44\", 8), rep(\"45-64\", 8), rep(\"65+\", 8)), 50),\n                      StratificationCategory1=rep(rep(c(rep(\"Disability Status\", 2), rep(\"Disability Type\", 6)),3), 50),\n                      Stratification1=rep(c(rep(c(\"Any Disability\", \"No Disability\", \"Cognitive Disability\", \"Hearing Disability\", \"Independent Living Disability\", \"Mobility Disability\", \"Self-care Disability\", \"Vision Disability\"),3)),50),\n                      Data_Value=NA)\n\n# Data frame for simulation data: gender\nsim_gender &lt;- data.frame(Response=rep(c(rep(\"Male\", 8), rep(\"Female\", 8)), 50),\n                         StratificationCategory1=rep(rep(c(rep(\"Disability Status\", 2), rep(\"Disability Type\", 6)),2), 50),\n                         Stratification1=rep(c(rep(c(\"Any Disability\", \"No Disability\", \"Cognitive Disability\", \"Hearing Disability\", \"Independent Living Disability\", \"Mobility Disability\", \"Self-care Disability\", \"Vision Disability\"),2)),50),\n                         Data_Value=NA)\n\n# Data frame for simulation data: race\nsim_race &lt;- data.frame(Response=rep(c(rep(\"Hispanic\", 8), rep(\"White, non-Hispanic\", 8), rep(\"Black, non-Hispanic\", 8), rep(\"American Indian or Alaska Native, non-Hispanic\", 8), rep(\"Asian, non-Hispanic\", 8), rep(\"Native Hawaiian or Other Pacific Islander, non-Hispanic\", 8), rep(\"Other / Multirace, non-Hispanic\", 8)), 50),\n                       StratificationCategory1=rep(rep(c(rep(\"Disability Status\", 2), rep(\"Disability Type\", 6)),7), 50),\n                       Stratification1=rep(c(rep(c(\"Any Disability\", \"No Disability\", \"Cognitive Disability\", \"Hearing Disability\", \"Independent Living Disability\", \"Mobility Disability\", \"Self-care Disability\", \"Vision Disability\"),7)),50),\n                       Data_Value=NA)\n\n# Data frame for simulation data: veteran status\nsim_vet &lt;- data.frame(Response=rep(c(rep(\"Veteran\", 8), rep(\"Non-Veteran\", 8)), 50),\n                      StratificationCategory1=rep(rep(c(rep(\"Disability Status\", 2), rep(\"Disability Type\", 6)),2), 50),\n                      Stratification1=rep(c(rep(c(\"Any Disability\", \"No Disability\", \"Cognitive Disability\", \"Hearing Disability\", \"Independent Living Disability\", \"Mobility Disability\", \"Self-care Disability\", \"Vision Disability\"),2)),50),\n                      Data_Value=NA)\n\nNow I will simulate the distribution of prevalence within different demographic factors. Let’s start with different age groups. Below is the histogram of prevalence among those who are 18-44 years old with different disability status or types.\n\n# Histograms for age group 18-44\ncdcdata_final %&gt;%\n  filter(Response==\"18-44\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 10 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nNow I will fit the distributions of prevalence for the 8 categories. As shown, most of the prevalence records have a skewed distribution. For ease of report, I will just fit a normal distribution to each of them.\n\n# Factorize Stratification1\ncdcdata_final$Stratification1 &lt;- factor(cdcdata_final$Stratification1, levels=unique(cdcdata_final$Stratification1))\n\n\n# Fit normal distribution\ndata_1844 &lt;- cdcdata_final %&gt;%\n  filter(Response==\"18-44\") %&gt;%\n  drop_na(Data_Value)\n\n# Parameters for normal distribution\nnorm_1844 &lt;- data.frame(Str=levels(cdcdata_final$Stratification1),\n                        mean=rep(NA, length(levels(cdcdata_final$Stratification1))),\n                        sd=rep(NA, length(levels(cdcdata_final$Stratification1))))\n\n# Run a loop to estimate parameters\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  x &lt;- data_1844[data_1844$Stratification1==norm_1844$Str[i],]$Data_Value\n  fit &lt;- fitdist(x, \"norm\")\n  norm_1844$mean[i] &lt;- fit$estimate[1]\n  norm_1844$sd[i] &lt;- fit$estimate[2]\n}\n\n# Output\nnorm_1844\n\n                            Str      mean        sd\n1                 No Disability 78.482000 2.8923824\n2                Any Disability 21.518000 2.8923824\n3           Mobility Disability  4.566000 1.0011214\n4 Independent Living Disability  6.102000 1.1927263\n5          Cognitive Disability 14.818000 2.1057721\n6            Hearing Disability  2.753061 0.7882044\n7          Self-care Disability  1.821951 0.4926346\n8             Vision Disability  3.200000 1.0129166\n\n\nThen we can use these estimates to simulate normally distributed data. I will write a loop to fill in the Data_Value column in sim_age.\n\n# Simulate: age 18-44\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  sim_age[which(sim_age$Response==\"18-44\" & sim_age$Stratification1==levels(cdcdata_final$Stratification1)[i]),]$Data_Value &lt;- rnorm(n=sum(sim_age$Response==\"18-44\" & sim_age$Stratification1==levels(cdcdata_final$Stratification1)[i]), mean=norm_1844$mean[i], sd=norm_1844$sd[i])\n}\n\nSimilarly, let’s simulate normally distributed data for those aged 45-64 years or 65+ years. Below I’m showing the histograms for those aged 45-64 years old.\n\n# Histograms for age group 45-64\ncdcdata_final %&gt;%\n  filter(Response==\"45-64\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNow let’s estimate the parameters and simulate data for this group.\n\n# Fit normal distribution\ndata_4564 &lt;- cdcdata_final %&gt;%\n  filter(Response==\"45-64\") %&gt;%\n  drop_na(Data_Value)\n\n# Parameters for normal distribution\nnorm_4564 &lt;- data.frame(Str=levels(cdcdata_final$Stratification1),\n                        mean=rep(NA, length(levels(cdcdata_final$Stratification1))),\n                        sd=rep(NA, length(levels(cdcdata_final$Stratification1))))\n\n# Run a loop to estimate parameters\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  x &lt;- data_4564[data_4564$Stratification1==norm_4564$Str[i],]$Data_Value\n  fit &lt;- fitdist(x, \"norm\")\n  norm_4564$mean[i] &lt;- fit$estimate[1]\n  norm_4564$sd[i] &lt;- fit$estimate[2]\n}\n\n# Output\nnorm_4564\n\n                            Str   mean       sd\n1                 No Disability 71.414 5.765310\n2                Any Disability 28.586 5.765310\n3           Mobility Disability 16.772 4.734049\n4 Independent Living Disability  7.470 2.332488\n5          Cognitive Disability 11.518 2.989628\n6            Hearing Disability  6.698 1.828605\n7          Self-care Disability  5.110 1.594396\n8             Vision Disability  5.652 1.848809\n\n\n\n# Simulate: age 45-64\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  sim_age[which(sim_age$Response==\"45-64\" & sim_age$Stratification1==levels(cdcdata_final$Stratification1)[i]),]$Data_Value &lt;- rnorm(n=sum(sim_age$Response==\"45-64\" & sim_age$Stratification1==levels(cdcdata_final$Stratification1)[i]), mean=norm_4564$mean[i], sd=norm_4564$sd[i])\n}\n\nBelow are the histograms for those aged 65+ years old.\n\n# Histograms for age group 65+\ncdcdata_final %&gt;%\n  filter(Response==\"65+\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nGood, let’s simulate.\n\n# Fit normal distribution\ndata_65 &lt;- cdcdata_final %&gt;%\n  filter(Response==\"65+\") %&gt;%\n  drop_na(Data_Value)\n\n# Parameters for normal distribution\nnorm_65 &lt;- data.frame(Str=levels(cdcdata_final$Stratification1),\n                      mean=rep(NA, length(levels(cdcdata_final$Stratification1))),\n                      sd=rep(NA, length(levels(cdcdata_final$Stratification1))))\n\n# Run a loop to estimate parameters\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  x &lt;- data_65[data_65$Stratification1==norm_65$Str[i],]$Data_Value\n  fit &lt;- fitdist(x, \"norm\")\n  norm_65$mean[i] &lt;- fit$estimate[1]\n  norm_65$sd[i] &lt;- fit$estimate[2]\n}\n\n# Output\nnorm_65\n\n                            Str   mean       sd\n1                 No Disability 56.646 5.537227\n2                Any Disability 43.354 5.537227\n3           Mobility Disability 26.608 4.859993\n4 Independent Living Disability  9.708 2.603524\n5          Cognitive Disability  9.804 2.691985\n6            Hearing Disability 17.188 3.046220\n7          Self-care Disability  5.718 1.763371\n8             Vision Disability  7.164 1.880825\n\n\n\n# Simulate: age 45-64\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  sim_age[which(sim_age$Response==\"65+\" & sim_age$Stratification1==levels(cdcdata_final$Stratification1)[i]),]$Data_Value &lt;- rnorm(n=sum(sim_age$Response==\"65+\" & sim_age$Stratification1==levels(cdcdata_final$Stratification1)[i]), mean=norm_65$mean[i], sd=norm_65$sd[i])\n}\n\nSo far so good. Let’s repeat what we have done for the other three demographic factors: gender, race, and veteran status. Before we get started, let’s look at the histograms for males and females.\n\n# Histograms for male\ncdcdata_final %&gt;%\n  filter(Response==\"Male\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n# Histograms for female\ncdcdata_final %&gt;%\n  filter(Response==\"Female\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nLooks good! Let’s estimate the normal distribution parameters for males and females.\n\n# Fit normal distribution\ndata_male &lt;- cdcdata_final %&gt;%\n  filter(Response==\"Male\") %&gt;%\n  drop_na(Data_Value)\n\n# Parameters for normal distribution\nnorm_male &lt;- data.frame(Str=levels(cdcdata_final$Stratification1),\n                        mean=rep(NA, length(levels(cdcdata_final$Stratification1))),\n                        sd=rep(NA, length(levels(cdcdata_final$Stratification1))))\n\n# Run a loop to estimate parameters\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  x &lt;- data_male[data_male$Stratification1==norm_male$Str[i],]$Data_Value\n  fit &lt;- fitdist(x, \"norm\")\n  norm_male$mean[i] &lt;- fit$estimate[1]\n  norm_male$sd[i] &lt;- fit$estimate[2]\n}\n\n# Output\nnorm_male\n\n                            Str   mean       sd\n1                 No Disability 74.194 3.927514\n2                Any Disability 25.806 3.927514\n3           Mobility Disability 10.324 2.418310\n4 Independent Living Disability  5.480 1.313621\n5          Cognitive Disability 11.286 2.063396\n6            Hearing Disability  8.158 2.058455\n7          Self-care Disability  3.386 1.014694\n8             Vision Disability  4.296 1.291195\n\n\n\n# Simulate: male\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  sim_gender[which(sim_gender$Response==\"Male\" & sim_gender$Stratification1==levels(cdcdata_final$Stratification1)[i]),]$Data_Value &lt;- rnorm(n=sum(sim_gender$Response==\"Male\" & sim_gender$Stratification1==levels(cdcdata_final$Stratification1)[i]), mean=norm_male$mean[i], sd=norm_male$sd[i])\n}\n\nSimilar steps for females.\n\n# Fit normal distribution\ndata_female &lt;- cdcdata_final %&gt;%\n  filter(Response==\"Female\") %&gt;%\n  drop_na(Data_Value)\n\n# Parameters for normal distribution\nnorm_female &lt;- data.frame(Str=levels(cdcdata_final$Stratification1),\n                        mean=rep(NA, length(levels(cdcdata_final$Stratification1))),\n                        sd=rep(NA, length(levels(cdcdata_final$Stratification1))))\n\n# Run a loop to estimate parameters\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  x &lt;- data_female[data_female$Stratification1==norm_female$Str[i],]$Data_Value\n  fit &lt;- fitdist(x, \"norm\")\n  norm_female$mean[i] &lt;- fit$estimate[1]\n  norm_female$sd[i] &lt;- fit$estimate[2]\n}\n\n# Output\nnorm_female\n\n                            Str   mean        sd\n1                 No Disability 71.090 4.1874694\n2                Any Disability 28.910 4.1874694\n3           Mobility Disability 13.500 2.8775684\n4 Independent Living Disability  8.710 1.7026156\n5          Cognitive Disability 14.666 2.6530443\n6            Hearing Disability  4.758 0.9577244\n7          Self-care Disability  3.578 0.8273548\n8             Vision Disability  4.922 1.3733594\n\n\n\n# Simulate: female\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  sim_gender[which(sim_gender$Response==\"Female\" & sim_gender$Stratification1==levels(cdcdata_final$Stratification1)[i]),]$Data_Value &lt;- rnorm(n=sum(sim_gender$Response==\"Female\" & sim_gender$Stratification1==levels(cdcdata_final$Stratification1)[i]), mean=norm_female$mean[i], sd=norm_female$sd[i])\n}\n\nNext, I will generate simulated data for different race groups. But still, let’s look at the histograms first.\n\n# Histograms for different race groups\ncdcdata_final %&gt;%\n  filter(Response==\"Hispanic\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 108 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n# Histograms for different race groups\ncdcdata_final %&gt;%\n  filter(Response==\"White, non-Hispanic\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n# Histograms for different race groups\ncdcdata_final %&gt;%\n  filter(Response==\"Black, non-Hispanic\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 121 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n# Histograms for different race groups\ncdcdata_final %&gt;%\n  filter(Response==\"American Indian or Alaska Native, non-Hispanic\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 253 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n# Histograms for different race groups\ncdcdata_final %&gt;%\n  filter(Response==\"Asian, non-Hispanic\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 309 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n# Histograms for different race groups\ncdcdata_final %&gt;%\n  filter(Response==\"Native Hawaiian or Other Pacific Islander, non-Hispanic\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 390 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n# Histograms for different race groups\ncdcdata_final %&gt;%\n  filter(Response==\"Other / Multirace, non-Hispanic\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 127 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nOf note, some of the distributions are very sparse, this could cause some parameter estimation problems. Let’s run the regular steps and see what will happen.\n\n# Fit normal distribution\ndata_Hispanic &lt;- cdcdata_final %&gt;%\n  filter(Response==\"Hispanic\") %&gt;%\n  drop_na(Data_Value)\n\n# Parameters for normal distribution\nnorm_Hispanic &lt;- data.frame(Str=levels(cdcdata_final$Stratification1),\n                            mean=rep(NA, length(levels(cdcdata_final$Stratification1))),\n                            sd=rep(NA, length(levels(cdcdata_final$Stratification1))))\n\n# Run a loop to estimate parameters\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  x &lt;- data_Hispanic[data_Hispanic$Stratification1==norm_Hispanic$Str[i],]$Data_Value\n  fit &lt;- fitdist(x, \"norm\")\n  norm_Hispanic$mean[i] &lt;- fit$estimate[1]\n  norm_Hispanic$sd[i] &lt;- fit$estimate[2]\n}\n\n# Output\nnorm_Hispanic\n\n                            Str      mean       sd\n1                 No Disability 69.059184 5.000731\n2                Any Disability 30.940816 5.000731\n3           Mobility Disability 14.574419 3.847868\n4 Independent Living Disability  8.491429 2.818649\n5          Cognitive Disability 15.451111 3.719177\n6            Hearing Disability  6.662963 1.827597\n7          Self-care Disability  4.670588 1.786803\n8             Vision Disability  8.037037 2.420307\n\n\n\n# Simulate: race Hispanic\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  sim_race[which(sim_race$Response==\"Hispanic\" & sim_race$Stratification1==levels(cdcdata_final$Stratification1)[i]),]$Data_Value &lt;- rnorm(n=sum(sim_race$Response==\"Hispanic\" & sim_race$Stratification1==levels(cdcdata_final$Stratification1)[i]), mean=norm_Hispanic$mean[i], sd=norm_Hispanic$sd[i])\n}\n\n\n# Fit normal distribution\ndata_white &lt;- cdcdata_final %&gt;%\n  filter(Response==\"White, non-Hispanic\") %&gt;%\n  drop_na(Data_Value)\n\n# Parameters for normal distribution\nnorm_white &lt;- data.frame(Str=levels(cdcdata_final$Stratification1),\n                            mean=rep(NA, length(levels(cdcdata_final$Stratification1))),\n                            sd=rep(NA, length(levels(cdcdata_final$Stratification1))))\n\n# Run a loop to estimate parameters\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  x &lt;- data_white[data_white$Stratification1==norm_white$Str[i],]$Data_Value\n  fit &lt;- fitdist(x, \"norm\")\n  norm_white$mean[i] &lt;- fit$estimate[1]\n  norm_white$sd[i] &lt;- fit$estimate[2]\n}\n\n# Output\nnorm_white\n\n                            Str   mean        sd\n1                 No Disability 73.704 4.8066604\n2                Any Disability 26.296 4.8066604\n3           Mobility Disability 11.040 2.9465234\n4 Independent Living Disability  6.824 1.6749400\n5          Cognitive Disability 12.794 2.6718840\n6            Hearing Disability  6.386 1.5566644\n7          Self-care Disability  3.028 0.8620998\n8             Vision Disability  3.692 1.2749651\n\n\n\n# Simulate: race White\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  sim_race[which(sim_race$Response==\"White, non-Hispanic\" & sim_race$Stratification1==levels(cdcdata_final$Stratification1)[i]),]$Data_Value &lt;- rnorm(n=sum(sim_race$Response==\"White, non-Hispanic\" & sim_race$Stratification1==levels(cdcdata_final$Stratification1)[i]), mean=norm_white$mean[i], sd=norm_white$sd[i])\n}\n\n\n# Fit normal distribution\ndata_black &lt;- cdcdata_final %&gt;%\n  filter(Response==\"Black, non-Hispanic\") %&gt;%\n  drop_na(Data_Value)\n\n# Parameters for normal distribution\nnorm_black &lt;- data.frame(Str=levels(cdcdata_final$Stratification1),\n                            mean=rep(NA, length(levels(cdcdata_final$Stratification1))),\n                            sd=rep(NA, length(levels(cdcdata_final$Stratification1))))\n\n# Run a loop to estimate parameters\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  x &lt;- data_black[data_black$Stratification1==norm_black$Str[i],]$Data_Value\n  fit &lt;- fitdist(x, \"norm\")\n  norm_black$mean[i] &lt;- fit$estimate[1]\n  norm_black$sd[i] &lt;- fit$estimate[2]\n}\n\n# Output\nnorm_black\n\n                            Str      mean       sd\n1                 No Disability 70.868889 5.095219\n2                Any Disability 29.316279 5.116864\n3           Mobility Disability 15.935135 3.345527\n4 Independent Living Disability  8.866667 2.108089\n5          Cognitive Disability 14.389474 2.498293\n6            Hearing Disability  4.722222 1.349714\n7          Self-care Disability  5.261538 1.014918\n8             Vision Disability  6.816667 1.540148\n\n\n\n# Simulate: race Black\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  sim_race[which(sim_race$Response==\"Black, non-Hispanic\" & sim_race$Stratification1==levels(cdcdata_final$Stratification1)[i]),]$Data_Value &lt;- rnorm(n=sum(sim_race$Response==\"Black, non-Hispanic\" & sim_race$Stratification1==levels(cdcdata_final$Stratification1)[i]), mean=norm_black$mean[i], sd=norm_black$sd[i])\n}\n\n\n# Fit normal distribution\ndata_AI &lt;- cdcdata_final %&gt;%\n  filter(Response==\"American Indian or Alaska Native, non-Hispanic\") %&gt;%\n  drop_na(Data_Value)\n\n# Parameters for normal distribution\nnorm_AI &lt;- data.frame(Str=levels(cdcdata_final$Stratification1),\n                            mean=rep(NA, length(levels(cdcdata_final$Stratification1))),\n                            sd=rep(NA, length(levels(cdcdata_final$Stratification1))))\n\n# Run a loop to estimate parameters\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  x &lt;- data_AI[data_AI$Stratification1==norm_AI$Str[i],]$Data_Value\n  fit &lt;- fitdist(x, \"norm\")\n  norm_AI$mean[i] &lt;- fit$estimate[1]\n  norm_AI$sd[i] &lt;- fit$estimate[2]\n}\n\n# Output\nnorm_AI\n\n                            Str     mean       sd\n1                 No Disability 58.44516 6.234547\n2                Any Disability 41.55484 6.234547\n3           Mobility Disability 19.23077 3.585630\n4 Independent Living Disability 10.94167 3.066610\n5          Cognitive Disability 19.92105 5.547648\n6            Hearing Disability 11.97143 5.947320\n7          Self-care Disability  5.45000 1.516300\n8             Vision Disability  9.57500 2.771169\n\n\n\n# Simulate: race AI\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  sim_race[which(sim_race$Response==\"American Indian or Alaska Native, non-Hispanic\" & sim_race$Stratification1==levels(cdcdata_final$Stratification1)[i]),]$Data_Value &lt;- rnorm(n=sum(sim_race$Response==\"American Indian or Alaska Native, non-Hispanic\" & sim_race$Stratification1==levels(cdcdata_final$Stratification1)[i]), mean=norm_AI$mean[i], sd=norm_AI$sd[i])\n}\n\n\n# Fit normal distribution\ndata_AS &lt;- cdcdata_final %&gt;%\n  filter(Response==\"Asian, non-Hispanic\") %&gt;%\n  drop_na(Data_Value)\n\n# Parameters for normal distribution\nnorm_AS &lt;- data.frame(Str=levels(cdcdata_final$Stratification1),\n                            mean=rep(NA, length(levels(cdcdata_final$Stratification1))),\n                            sd=rep(NA, length(levels(cdcdata_final$Stratification1))))\n\n# Run a loop to estimate parameters\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  x &lt;- data_AS[data_AS$Stratification1==norm_AS$Str[i],]$Data_Value\n  fit &lt;- fitdist(x, \"norm\")\n  norm_AS$mean[i] &lt;- fit$estimate[1]\n  norm_AS$sd[i] &lt;- fit$estimate[2]\n}\n\n# Output\nnorm_AS\n\n                            Str      mean        sd\n1                 No Disability 83.486111 4.2571608\n2                Any Disability 17.110345 4.2845629\n3           Mobility Disability  7.100000 2.9400680\n4 Independent Living Disability  4.666667 0.8806563\n5          Cognitive Disability  8.433333 3.1513665\n6            Hearing Disability  4.266667 1.1323525\n7          Self-care Disability  2.700000 1.3000000\n8             Vision Disability  6.875000 6.2515498\n\n\n\n# Simulate: race AS\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  sim_race[which(sim_race$Response==\"Asian, non-Hispanic\" & sim_race$Stratification1==levels(cdcdata_final$Stratification1)[i]),]$Data_Value &lt;- rnorm(n=sum(sim_race$Response==\"Asian, non-Hispanic\" & sim_race$Stratification1==levels(cdcdata_final$Stratification1)[i]), mean=norm_AS$mean[i], sd=norm_AS$sd[i])\n}\n\nWhen I was trying to simulate data for Native Hawaiian or Other Pacific Islander, non-Hispanic samples, there is a problem that this subgroup has a too small sample size. So I will manually fill in the simulation data based on what we have from the original dataset. First let’s look how small this group is.\n\n# Fit normal distribution\ndata_PI &lt;- cdcdata_final %&gt;%\n  filter(Response==\"Native Hawaiian or Other Pacific Islander, non-Hispanic\") %&gt;%\n  drop_na(Data_Value)\ndata_PI\n\n# A tibble: 10 × 9\n   LocationDesc Category             Indicator   Response StratificationCatego…¹\n   &lt;chr&gt;        &lt;chr&gt;                &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;                 \n 1 Hawaii       Disability Estimates Disability… Native … Disability Type       \n 2 Hawaii       Disability Estimates Disability… Native … Disability Status     \n 3 Hawaii       Disability Estimates Disability… Native … Disability Status     \n 4 Hawaii       Disability Estimates Disability… Native … Disability Type       \n 5 Hawaii       Disability Estimates Disability… Native … Disability Type       \n 6 Hawaii       Disability Estimates Disability… Native … Disability Type       \n 7 Hawaii       Disability Estimates Disability… Native … Disability Type       \n 8 Washington   Disability Estimates Disability… Native … Disability Type       \n 9 Washington   Disability Estimates Disability… Native … Disability Status     \n10 Washington   Disability Estimates Disability… Native … Disability Status     \n# ℹ abbreviated name: ¹​StratificationCategory1\n# ℹ 4 more variables: Stratification1 &lt;fct&gt;, Data_Value &lt;dbl&gt;,\n#   disability_yes &lt;chr&gt;, Ages &lt;chr&gt;\n\n\nAs shown, there are only 10 rows. To make my life easier, I will just fill in the simulation dataset with the mean of Data_Value in these sub-categories.\n\n# Fill in simulation data manually\nsim_race[which(sim_race$Response==\"Native Hawaiian or Other Pacific Islander, non-Hispanic\" & \n                 sim_race$Stratification1==\"Any Disability\"),]$Data_Value &lt;- \n  (27.8+24.5)/2\n\nsim_race[which(sim_race$Response==\"Native Hawaiian or Other Pacific Islander, non-Hispanic\" & \n                 sim_race$Stratification1==\"No Disability\"),]$Data_Value &lt;- \n  (72.2+75.5)/2\n\nsim_race[which(sim_race$Response==\"Native Hawaiian or Other Pacific Islander, non-Hispanic\" & \n                 sim_race$Stratification1==\"Cognitive Disability\"),]$Data_Value &lt;- 9.8\n\nsim_race[which(sim_race$Response==\"Native Hawaiian or Other Pacific Islander, non-Hispanic\" & \n                 sim_race$Stratification1==\"Hearing Disability\"),]$Data_Value &lt;- 7.6\n\nsim_race[which(sim_race$Response==\"Native Hawaiian or Other Pacific Islander, non-Hispanic\" & \n                 sim_race$Stratification1==\"Independent Living Disability\"),]$Data_Value &lt;- 6.3\n\nsim_race[which(sim_race$Response==\"Native Hawaiian or Other Pacific Islander, non-Hispanic\" & \n                 sim_race$Stratification1==\"Mobility Disability\"),]$Data_Value &lt;- \n  (12.4+18.1)/2\n\nsim_race[which(sim_race$Response==\"Native Hawaiian or Other Pacific Islander, non-Hispanic\" & \n                 sim_race$Stratification1==\"Self-care Disability\"),]$Data_Value &lt;- 0\n\nsim_race[which(sim_race$Response==\"Native Hawaiian or Other Pacific Islander, non-Hispanic\" & \n                 sim_race$Stratification1==\"Vision Disability\"),]$Data_Value &lt;- 3.6\n\nThen, let’s run the regular steps for Other / Multirace, non-Hispanic.\n\n# Fit normal distribution\ndata_other &lt;- cdcdata_final %&gt;%\n  filter(Response==\"Other / Multirace, non-Hispanic\") %&gt;%\n  drop_na(Data_Value)\n\n# Parameters for normal distribution\nnorm_other &lt;- data.frame(Str=levels(cdcdata_final$Stratification1),\n                            mean=rep(NA, length(levels(cdcdata_final$Stratification1))),\n                            sd=rep(NA, length(levels(cdcdata_final$Stratification1))))\n\n# Run a loop to estimate parameters\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  x &lt;- data_other[data_other$Stratification1==norm_other$Str[i],]$Data_Value\n  fit &lt;- fitdist(x, \"norm\")\n  norm_other$mean[i] &lt;- fit$estimate[1]\n  norm_other$sd[i] &lt;- fit$estimate[2]\n}\n\n# Output\nnorm_other\n\n                            Str      mean       sd\n1                 No Disability 63.420000 7.688485\n2                Any Disability 36.580000 7.688485\n3           Mobility Disability 16.668182 4.746136\n4 Independent Living Disability 10.490000 3.560932\n5          Cognitive Disability 18.680488 5.562120\n6            Hearing Disability  9.153571 3.519891\n7          Self-care Disability  5.441667 1.368368\n8             Vision Disability  8.044444 3.394804\n\n\n\n# Simulate: race other\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  sim_race[which(sim_race$Response==\"Other / Multirace, non-Hispanic\" & sim_race$Stratification1==levels(cdcdata_final$Stratification1)[i]),]$Data_Value &lt;- rnorm(n=sum(sim_race$Response==\"Other / Multirace, non-Hispanic\" & sim_race$Stratification1==levels(cdcdata_final$Stratification1)[i]), mean=norm_other$mean[i], sd=norm_other$sd[i])\n}\n\nFinally, let’s simulate prevalence records based on veteran status. Let’s take a look at the histograms first.\n\n# Histograms for veteran\ncdcdata_final %&gt;%\n  filter(Response==\"Veteran\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 34 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n# Histograms for non-veteran\ncdcdata_final %&gt;%\n  filter(Response==\"Non-Veteran\") %&gt;%\n  ggplot(aes(x=Data_Value))+\n  geom_histogram(fill=\"firebrick4\", color=\"black\", alpha=.8)+\n  facet_wrap(~Stratification1, ncol=4, scales=\"free\")+\n  labs(x=\"Prevalence\", y=\"Count\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.title.y=element_text(size=15, color=\"black\", face=\"bold\"),\n        axis.text.x=element_text(color=\"black\", size=9),\n        axis.text.y=element_text(color=\"black\", size=9))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nGood! Let’s estimate the normal distribution parameters for veterans and non-veterans.\n\n# Fit normal distribution\ndata_vet &lt;- cdcdata_final %&gt;%\n  filter(Response==\"Veteran\") %&gt;%\n  drop_na(Data_Value)\n\n# Parameters for normal distribution\nnorm_vet &lt;- data.frame(Str=levels(cdcdata_final$Stratification1),\n                        mean=rep(NA, length(levels(cdcdata_final$Stratification1))),\n                        sd=rep(NA, length(levels(cdcdata_final$Stratification1))))\n\n# Run a loop to estimate parameters\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  x &lt;- data_vet[data_vet$Stratification1==norm_vet$Str[i],]$Data_Value\n  fit &lt;- fitdist(x, \"norm\")\n  norm_vet$mean[i] &lt;- fit$estimate[1]\n  norm_vet$sd[i] &lt;- fit$estimate[2]\n}\n\n# Output\nnorm_vet\n\n                            Str      mean       sd\n1                 No Disability 69.060000 4.494708\n2                Any Disability 30.940000 4.494708\n3           Mobility Disability 13.565306 2.728500\n4 Independent Living Disability  6.968889 1.896409\n5          Cognitive Disability 13.638776 2.919590\n6            Hearing Disability 12.200000 3.299165\n7          Self-care Disability  4.305405 1.140163\n8             Vision Disability  3.762162 1.055834\n\n\n\n# Simulate: veteran\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  sim_vet[which(sim_vet$Response==\"Veteran\" & sim_vet$Stratification1==levels(cdcdata_final$Stratification1)[i]),]$Data_Value &lt;- rnorm(n=sum(sim_vet$Response==\"Veteran\" & sim_vet$Stratification1==levels(cdcdata_final$Stratification1)[i]), mean=norm_vet$mean[i], sd=norm_vet$sd[i])\n}\n\nSimilar steps for non-veterans.\n\n# Fit normal distribution\ndata_nvet &lt;- cdcdata_final %&gt;%\n  filter(Response==\"Non-Veteran\") %&gt;%\n  drop_na(Data_Value)\n\n# Parameters for normal distribution\nnorm_nvet &lt;- data.frame(Str=levels(cdcdata_final$Stratification1),\n                        mean=rep(NA, length(levels(cdcdata_final$Stratification1))),\n                        sd=rep(NA, length(levels(cdcdata_final$Stratification1))))\n\n# Run a loop to estimate parameters\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  x &lt;- data_nvet[data_nvet$Stratification1==norm_nvet$Str[i],]$Data_Value\n  fit &lt;- fitdist(x, \"norm\")\n  norm_nvet$mean[i] &lt;- fit$estimate[1]\n  norm_nvet$sd[i] &lt;- fit$estimate[2]\n}\n\n# Output\nnorm_nvet\n\n                            Str   mean        sd\n1                 No Disability 73.140 4.0221387\n2                Any Disability 26.860 4.0221387\n3           Mobility Disability 11.850 2.6821074\n4 Independent Living Disability  7.198 1.4800662\n5          Cognitive Disability 12.910 2.2699119\n6            Hearing Disability  5.600 1.1914697\n7          Self-care Disability  3.370 0.8478797\n8             Vision Disability  4.654 1.3084663\n\n\n\n# Simulate: non-veteran\nfor(i in 1:length(levels(cdcdata_final$Stratification1))){\n  sim_vet[which(sim_vet$Response==\"Non-Veteran\" & sim_vet$Stratification1==levels(cdcdata_final$Stratification1)[i]),]$Data_Value &lt;- rnorm(n=sum(sim_vet$Response==\"Non-Veteran\" & sim_vet$Stratification1==levels(cdcdata_final$Stratification1)[i]), mean=norm_nvet$mean[i], sd=norm_nvet$sd[i])\n}\n\n\n\nValidation\nI will just copy the primary author’s code to make similar plots to check if the simulated data generally follow the same pattern. First of all, let’s look at the grouped boxplots for the three age groups.\n\n# Grouped box plot for 18-44\nsim_age %&gt;%\n  filter(Response==\"18-44\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in ages 18-44 in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\n\n# Grouped box plot for 45-64\nsim_age %&gt;%\n  filter(Response==\"45-64\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in ages 45-64 in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\n\n# Grouped box plot for 65+\nsim_age %&gt;%\n  filter(Response==\"65+\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in ages 65+ in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\nLooks perfect! The simulated prevalence records follows exactly the pattern as the original data! Now Let’s take a look at the pattern grouped by gender.\n\n# Grouped box plot for male\nsim_gender %&gt;%\n  filter(Response==\"Male\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in males in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\n\n# Grouped box plot for female\nsim_gender %&gt;%\n  filter(Response==\"Female\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in females in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\nAlso looks good. Then let’s try race groups.\n\n# Grouped box plot for Hispanic\nsim_race %&gt;%\n  filter(Response==\"Hispanic\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in Hispanic in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\n\n# Grouped box plot for White, non-Hispanic\nsim_race %&gt;%\n  filter(Response==\"White, non-Hispanic\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in White in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\n\n# Grouped box plot for Black, non-Hispanic\nsim_race %&gt;%\n  filter(Response==\"Black, non-Hispanic\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in Black in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\n\n# Grouped box plot for American Indian or Alaska Native, non-Hispanic\nsim_race %&gt;%\n  filter(Response==\"American Indian or Alaska Native, non-Hispanic\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in American Indian or Alaska Native in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\n\n# Grouped box plot for Asian, non-Hispanic\nsim_race %&gt;%\n  filter(Response==\"Asian, non-Hispanic\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in Asian in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\n\n# Grouped box plot for Native Hawaiian or Other Pacific Islander, non-Hispanic\nsim_race %&gt;%\n  filter(Response==\"Native Hawaiian or Other Pacific Islander, non-Hispanic\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in Native Hawaiian or Other Pacific Islander in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\n\n# Grouped box plot for Other / Multirace, non-Hispanic\nsim_race %&gt;%\n  filter(Response==\"Other / Multirace, non-Hispanic\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in Other / Multirace in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\nAll boxplots look good except the Native Hawaiian or Other Pacific Islander, non-Hispanic one. With such a small sample size for this group, there is very little I can do about it. Anyway, the overall patterns follows the patterns in original dataset perfectly.\nFinally, let’s look at the patterns involving veteran status.\n\n# Grouped box plot for veteran\nsim_vet %&gt;%\n  filter(Response==\"Veteran\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in veterans in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\n\n# Grouped box plot for non-veteran\nsim_vet %&gt;%\n  filter(Response==\"Non-Veteran\") %&gt;%\n  ggplot(aes(x=Stratification1 , y=Data_Value, color=Stratification1)) +\n  geom_boxplot() +\n  labs(title=\"Simulation: Disabilities in non-veterans in 2021\",\n       x=\"Disability Type\",\n       y=\"Prevelance\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\n\n\nOK, by far, all patterns in the original dataset has been simulated. As this only involves univariate analysis, I did not look at the association between variables, which could take another week for simulation :)"
  },
  {
    "objectID": "aboutme.html#interestshobbies",
    "href": "aboutme.html#interestshobbies",
    "title": "About me",
    "section": "Interests/hobbies",
    "text": "Interests/hobbies\nIn my free time I like to play video games.\nCurrent game is God of War\nI’m an avid baker, my home is constantly stocked with some sort of baked good. In contradiction to that, I am really interested in health and wellness and try to spend time in the gym or outdoors. :)\nI look forward to learning and adding to my portfolio!"
  },
  {
    "objectID": "aboutme.html#cool-data",
    "href": "aboutme.html#cool-data",
    "title": "About me",
    "section": "Cool data:",
    "text": "Cool data:\nI found this a few months ago looking for a coffee shop near me. Someone mapped out basically all of the coffee shops in the atl/metro atl area. I love coffee and cafes so it has definitely come in handy. Its color coded to quickly give an idea of what part of atl the shop is in. Maybe i’ll try to visit them all someday…\nhttps://www.atlantacoffeeshops.com/atlanta-coffee-map"
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Moduel 4 Exercise",
    "section": "",
    "text": "For this exercise I will be analyzing an Image. #The first step is to load in the necessary packages for exploring images. This will be magick and imager. Both packages can be used to manipulate the images properties such as its size, colors, effects, and even animating. Imager won’t run on MacOS unless you install XQuartz first here\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.12.93\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\nlibrary(imager)\n\nLoading required package: magrittr\n\nAttaching package: 'magrittr'\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\nAttaching package: 'imager'\n\nThe following object is masked from 'package:magrittr':\n\n    add\n\nThe following object is masked from 'package:stringr':\n\n    boundary\n\nThe following object is masked from 'package:dplyr':\n\n    where\n\nThe following object is masked from 'package:tidyr':\n\n    fill\n\nThe following objects are masked from 'package:stats':\n\n    convolve, spectrum\n\nThe following object is masked from 'package:graphics':\n\n    frame\n\nThe following object is masked from 'package:base':\n\n    save.image\n\n\n\nNow I will load in my images, my images are characters from a show so I will use their names\n\njjk &lt;- list(\n  itadori = image_read(\"/Users/alexisgonzalez/Downloads/itadori.jpg\"),\n  nobara = image_read(\"/Users/alexisgonzalez/Downloads/nobara.jpg\"),\n  megumi = image_read(\"/Users/alexisgonzalez/Downloads/megumi.jpg\"),\n  nanami = image_read(\"/Users/alexisgonzalez/Downloads/nanami.jpg\"),\n  gojo = image_read(\"/Users/alexisgonzalez/Downloads/gojo.jpg\"),\n  sukuna = image_read(\"/Users/alexisgonzalez/Downloads/sukuna.jpg\")\n  )\n\n\n\nThe sizes of the images are different so I’m going to make them all the same 640x640\n\nresized_images &lt;- lapply(jjk, function(img) {image_resize(img, \"640x640\" )})\n\n\n\nNow I can combine all of these images and turn them into a gif\n\ngif &lt;- image_animate(image_join(resized_images), fps = 5)\n\n\n\nNow I view my gif, it should play in the window on the bottom left\n\nprint(gif)\n\n# A tibble: 6 × 7\n  format width height colorspace matte filesize density\n  &lt;chr&gt;  &lt;int&gt;  &lt;int&gt; &lt;chr&gt;      &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;  \n1 gif      639    640 sRGB       FALSE        0 72x72  \n2 gif      639    640 sRGB       TRUE         0 72x72  \n3 gif      639    640 sRGB       TRUE         0 72x72  \n4 gif      639    640 sRGB       TRUE         0 72x72  \n5 gif      639    640 sRGB       TRUE         0 72x72  \n6 gif      639    640 sRGB       TRUE         0 72x72"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Placeholder file for the future R coding exercise.\n#adding in the dslabs package and other packages\nlibrary(dslabs)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/alexisgonzalez/Desktop/MADA/alexisgonzalez-MADA-portfolio\n#dslabs is now in and datasets are available\n#next is to read help file for gapminder\nhelp(gapminder)\n#get an overview of data structure and summary\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586\n#determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n#Assign african countries to new variable africadata\nafricadata &lt;- subset(gapminder, continent == \"Africa\")\n#Checking the new variable\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0\n#taking object africadata and creating mortlife (infant mortality x life expectancy) using the dplyr package\nmortlife &lt;- africadata %&gt;%\n  select(infant_mortality,life_expectancy)\n#check the new object\nstr(mortlife)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(mortlife)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226\n#repeat the same process but create new object poplife (population x life expectancy) using dplyr\npoplife &lt;- africadata %&gt;%\n  select(population,life_expectancy)\n#check new object poplife\nstr(poplife)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(poplife)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51\n#plotting life expectancy as a function of infant mortality \nggplot(mortlife, aes(x = life_expectancy, y = infant_mortality, color=\"blue\")) + geom_point()\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nfigure_file=here(\"coding-exercise\",\"lifeexpectancyxinfantmortality.png\")\nggsave(filename = figure_file)\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n#plotting population as a function of life expectancy\n# I used chatgpt to get the code for making the x variable on the log scale\nggplot(poplife, aes(x = population, y = life_expectancy, color=\"blue\")) + geom_point() + scale_x_log10()\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n figure_file=here(\"coding-exercise\",\"populationxlifeexpectancy.png\")\n#figuring out what years have missing infant mortality data\nmissing_data_years &lt;- africadata %&gt;%\n  filter(is.na(infant_mortality)) %&gt;%\n  select(year) %&gt;%\n  distinct()\nprint(missing_data_years)\n\n   year\n1  1960\n2  1961\n3  1962\n4  1963\n5  1964\n6  1965\n7  1966\n8  1967\n9  1968\n10 1969\n11 1970\n12 1971\n13 1972\n14 1973\n15 1974\n16 1975\n17 1976\n18 1977\n19 1978\n20 1979\n21 1980\n22 1981\n23 2016"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#this-following-section-is-contributed-by-vincent-nguyen",
    "href": "coding-exercise/coding-exercise.html#this-following-section-is-contributed-by-vincent-nguyen",
    "title": "R Coding Exercise",
    "section": "This following section is contributed by Vincent Nguyen",
    "text": "This following section is contributed by Vincent Nguyen\nThis analysis begins with loading and inspecting the data set, us_contagious_diseases\n\n# load in the package dslabs\nlibrary(dslabs)\n\n# load in the dataset, us_contagious_diseases\ndata(\"us_contagious_diseases\")\n\n# look at help file for data\nhelp(\"us_contagious_diseases\")\n\n# get an overview of data structure\nstr(us_contagious_diseases)\n\n'data.frame':   16065 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  50 49 52 49 51 51 45 45 45 46 ...\n $ count          : num  321 291 314 380 413 378 342 467 244 286 ...\n $ population     : num  3345787 3364130 3386068 3412450 3444165 ...\n\n# get a summary of the data\nsummary(us_contagious_diseases)\n\n        disease            state            year      weeks_reporting\n Hepatitis A:2346   Alabama   :  315   Min.   :1928   Min.   : 0.00  \n Measles    :3825   Alaska    :  315   1st Qu.:1950   1st Qu.:31.00  \n Mumps      :1785   Arizona   :  315   Median :1975   Median :46.00  \n Pertussis  :2856   Arkansas  :  315   Mean   :1971   Mean   :37.38  \n Polio      :2091   California:  315   3rd Qu.:1990   3rd Qu.:50.00  \n Rubella    :1887   Colorado  :  315   Max.   :2011   Max.   :52.00  \n Smallpox   :1275   (Other)   :14175                                 \n     count          population      \n Min.   :     0   Min.   :   86853  \n 1st Qu.:     7   1st Qu.: 1018755  \n Median :    69   Median : 2749249  \n Mean   :  1492   Mean   : 4107584  \n 3rd Qu.:   525   3rd Qu.: 4996229  \n Max.   :132342   Max.   :37607525  \n                  NA's   :214       \n\n# determine the type of object data is\nclass(us_contagious_diseases)\n\n[1] \"data.frame\"\n\n\nThis section of the analysis focuses on creating objects using the data set. The object, hepadata, is created to narrow the scope of the analysis to just Hepatitis A data. The objects, georgiadata and californiadata, are created to focus on Hepatitis A data in these states. Additionally, a new column is added to calculate incidence rates. Lastly, a dataframe is created, combining the data from both states. This is later used for another part of the analysis.\n\n# load tidyverse package\nlibrary(tidyverse)\n\n# create new object only containing pertussis data\nhepadata &lt;- us_contagious_diseases %&gt;%\n  filter(disease == \"Hepatitis A\")\n\n# create a new object containing data from only Georgia and create new incidence rate variable\ngeorgiadata &lt;- hepadata %&gt;%\n  filter(state == \"Georgia\") %&gt;%\n  mutate(incidence_rate = (count / population) * 100000)\n\n# create a new object containing data from only California and create new incidence rate variable\ncaliforniadata &lt;- hepadata %&gt;%\n  filter(state == \"California\")%&gt;%\n  mutate(incidence_rate = (count / population) * 100000)\n\n# create a new object containing both for comparisions later\n\n# first need to add new column so that each dataset indicates the state\ngeorgiadata$state &lt;- \"Georgia\"\ncaliforniadata$state &lt;- \"California\"\n\n# combine data sets into one\ncombined_data &lt;- rbind(georgiadata, californiadata)\n\nThis section creates a graph outlining the change in incidence rate over time in the state of GA and CA.\n\n# load package ggplot2\nlibrary(ggplot2)\n\n# create graph plotting year as x and incidence rate as y for Georgia\ngeorgia_incidence_graph &lt;- ggplot(georgiadata, aes(x = year, y = incidence_rate)) + geom_point(alpha = 0.6, color = \"blue\") + labs(\n  title = \"Incidence rate of Hep. A over time in Georgia\",\n  x = \"Time\",\n  y = \"Incidence (per 100,000\" )\n\n# display the newly created graph\nprint(georgia_incidence_graph)\n\n\n\n\n\n\n\n# create graph plotting year as x and incidence rate as y for California\ncalifornia_incidence_graph &lt;- ggplot(californiadata, aes(x = year, y = incidence_rate)) + geom_point(alpha = 0.6, color = \"blue\") + labs(\n  title = \"Incidence rate of Hep. A over time in California\",\n  x = \"Time\",\n  y = \"Incidence (per 100,000\" )\n\n# display the newly created graph\nprint(california_incidence_graph)\n\n\n\n\n\n\n\n\nThis section creates a simple linear model to analyze the interaction of incidence rate over time across the two states.\n\n# linear model with interaction between year and state\nlm_model &lt;- lm(incidence_rate ~ year * state, data = combined_data)\nsummary(lm_model)\n\n\nCall:\nlm(formula = incidence_rate ~ year * state, data = combined_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.8860 -3.8762  0.1214  2.4986 19.5427 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.899e+03  1.196e+02  15.876  &lt; 2e-16 ***\nyear              -9.462e-01  6.014e-02 -15.732  &lt; 2e-16 ***\nstateGeorgia      -1.014e+03  1.691e+02  -5.998 4.33e-08 ***\nyear:stateGeorgia  5.054e-01  8.506e-02   5.942 5.53e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.415 on 88 degrees of freedom\nMultiple R-squared:  0.8088,    Adjusted R-squared:  0.8023 \nF-statistic: 124.1 on 3 and 88 DF,  p-value: &lt; 2.2e-16\n\n\nFrom these results, it can be concluded that Georgia has a significantly lower baseline incidence rate of Hepatitis A compared to California, and additionally, the change in incidence over time differs between the two states. The interaction term between year and state indicates that the change in incidence over time is significantly different between the two states. The incidence rate in California significantly decreases at a greater rate than Georgia indicating a possibly more effective response to Hepatitis A."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "Hello\n\nHi my name is Alexis\nWelcome to my website and data analysis portfolio.\n\nPlease use the Menu Bar above to look around.\nHave fun!"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html",
    "href": "fitting-exercise/fitting-exercise.html",
    "title": "Model Fitting Exercise",
    "section": "",
    "text": "#Loading in the usual packages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(ggplot2)\n\n#Load in the data\n\ndata_path &lt;- file.path(\"../fitting-exercise/data/Mavoglurant_A2121_nmpk.csv\")\ndrug_data &lt;- read.csv(data_path)\n\n#Checking if my data loaded in properly\n\nstr(drug_data)\n\n'data.frame':   2678 obs. of  17 variables:\n $ ID  : int  793 793 793 793 793 793 793 793 793 793 ...\n $ CMT : int  1 2 2 2 2 2 2 2 2 2 ...\n $ EVID: int  1 0 0 0 0 0 0 0 0 0 ...\n $ EVI2: int  1 0 0 0 0 0 0 0 0 0 ...\n $ MDV : int  1 0 0 0 0 0 0 0 0 0 ...\n $ DV  : num  0 491 605 556 310 237 147 101 72.4 52.6 ...\n $ LNDV: num  0 6.2 6.41 6.32 5.74 ...\n $ AMT : num  25 0 0 0 0 0 0 0 0 0 ...\n $ TIME: num  0 0.2 0.25 0.367 0.533 0.7 1.2 2.2 3.2 4.2 ...\n $ DOSE: num  25 25 25 25 25 25 25 25 25 25 ...\n $ OCC : int  1 1 1 1 1 1 1 1 1 1 ...\n $ RATE: int  75 0 0 0 0 0 0 0 0 0 ...\n $ AGE : int  42 42 42 42 42 42 42 42 42 42 ...\n $ SEX : int  1 1 1 1 1 1 1 1 1 1 ...\n $ RACE: int  2 2 2 2 2 2 2 2 2 2 ...\n $ WT  : num  94.3 94.3 94.3 94.3 94.3 94.3 94.3 94.3 94.3 94.3 ...\n $ HT  : num  1.77 1.77 1.77 1.77 1.77 ...\n\nsummary(drug_data)\n\n       ID             CMT             EVID              EVI2       \n Min.   :793.0   Min.   :1.000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:832.0   1st Qu.:2.000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :860.0   Median :2.000   Median :0.00000   Median :0.0000  \n Mean   :858.8   Mean   :1.926   Mean   :0.07394   Mean   :0.1613  \n 3rd Qu.:888.0   3rd Qu.:2.000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :915.0   Max.   :2.000   Max.   :1.00000   Max.   :4.0000  \n      MDV                DV               LNDV            AMT        \n Min.   :0.00000   Min.   :   0.00   Min.   :0.000   Min.   : 0.000  \n 1st Qu.:0.00000   1st Qu.:  23.52   1st Qu.:3.158   1st Qu.: 0.000  \n Median :0.00000   Median :  74.20   Median :4.306   Median : 0.000  \n Mean   :0.09373   Mean   : 179.93   Mean   :4.085   Mean   : 2.763  \n 3rd Qu.:0.00000   3rd Qu.: 283.00   3rd Qu.:5.645   3rd Qu.: 0.000  \n Max.   :1.00000   Max.   :1730.00   Max.   :7.456   Max.   :50.000  \n      TIME             DOSE            OCC             RATE       \n Min.   : 0.000   Min.   :25.00   Min.   :1.000   Min.   :  0.00  \n 1st Qu.: 0.583   1st Qu.:25.00   1st Qu.:1.000   1st Qu.:  0.00  \n Median : 2.250   Median :37.50   Median :1.000   Median :  0.00  \n Mean   : 5.851   Mean   :37.37   Mean   :1.378   Mean   : 16.55  \n 3rd Qu.: 6.363   3rd Qu.:50.00   3rd Qu.:2.000   3rd Qu.:  0.00  \n Max.   :48.217   Max.   :50.00   Max.   :2.000   Max.   :300.00  \n      AGE            SEX             RACE              WT        \n Min.   :18.0   Min.   :1.000   Min.   : 1.000   Min.   : 56.60  \n 1st Qu.:26.0   1st Qu.:1.000   1st Qu.: 1.000   1st Qu.: 73.30  \n Median :31.0   Median :1.000   Median : 1.000   Median : 82.60  \n Mean   :32.9   Mean   :1.128   Mean   : 7.415   Mean   : 83.16  \n 3rd Qu.:40.0   3rd Qu.:1.000   3rd Qu.: 2.000   3rd Qu.: 90.60  \n Max.   :50.0   Max.   :2.000   Max.   :88.000   Max.   :115.30  \n       HT       \n Min.   :1.520  \n 1st Qu.:1.710  \n Median :1.780  \n Mean   :1.762  \n 3rd Qu.:1.820  \n Max.   :1.930  \n\n\n#Plotting DV by time and stratisfying by dose for each person\n\nggplot(drug_data, aes(x = TIME, y = DV, group = ID, color = as.factor(DOSE))) +\n  geom_line(alpha = 0.7) +\n  labs(x = \"TIME\", y = \"DV\", color = \"DOSE\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n#Removing all entries with OCC=2\n\ndrug_data2 &lt;- drug_data %&gt;%\n  filter(OCC != 2)\n\n#Removing time=0 and adding DV for each ID\n\ndrug_data3 &lt;- drug_data2 %&gt;%\n  filter(TIME != 0) %&gt;%\n  group_by(ID) %&gt;%\n  summarize(Y = sum(DV, na.rm = TRUE))\n\n#Data frame where time=0\n\ntime_zero &lt;- drug_data2 %&gt;%\n  filter(TIME==0)\n\n#Inner joining the data (maybe?)\n\ncombined_data &lt;- inner_join(drug_data3, time_zero, by = \"ID\")\n\n#Filtering the data more by removing OCC and EVID and converting RACE and SEX into factor variables\n\ndrug_data_final &lt;- combined_data %&gt;%\n  select(-OCC,-EVID) %&gt;%\n  mutate(\n    RACE = as.factor(RACE),\n    SEX = as.factor(SEX)\n  )\n\n#Exploratory Analysis #Summary tables\n\nsummary_table &lt;- drug_data_final %&gt;%\n  summarize(\n    n = n(),  # Number of observations\n    mean_Y = mean(Y, na.rm = TRUE),\n    sd_Y = sd(Y, na.rm = TRUE),\n    min_Y = min(Y, na.rm = TRUE),\n    max_Y = max(Y, na.rm = TRUE)\n  )\nprint(summary_table)\n\n# A tibble: 1 × 5\n      n mean_Y  sd_Y min_Y max_Y\n  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   120  2445.  962.  826. 5607.\n\n\n\nsummary_table &lt;- drug_data_final %&gt;%\n  summarize(\n    n = n(),  # Number of observations\n    mean_WT = mean(WT, na.rm = TRUE),\n    sd_WT = sd(WT, na.rm = TRUE),\n    min_WT = min(WT, na.rm = TRUE),\n    max_WT = max(WT, na.rm = TRUE)\n  )\nprint(summary_table)\n\n# A tibble: 1 × 5\n      n mean_WT sd_WT min_WT max_WT\n  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   120    82.6  12.5   56.6   115.\n\n\n\nsummary_table &lt;- drug_data_final %&gt;%\n  summarize(\n    n = n(),  # Number of observations\n    mean_HT = mean(HT, na.rm = TRUE),\n    sd_HT = sd(HT, na.rm = TRUE),\n    min_HT = min(HT, na.rm = TRUE),\n    max_HT = max(HT, na.rm = TRUE)\n  )\nprint(summary_table)\n\n# A tibble: 1 × 5\n      n mean_HT  sd_HT min_HT max_HT\n  &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   120    1.76 0.0855   1.52   1.93\n\n\n#Summary Figures #####plotting WT vs HT in a scatter plot\n\nggplot(drug_data_final, aes(x = WT, y = HT)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.7) + \n  labs(title = \"Scatter Plot of Weight vs Height\",\n       x = \"Weight (kg)\",\n       y = \"Height (cm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n#Distribution of dose across ages\n\nggplot(drug_data_final, aes(x = Y)) +\n  geom_histogram(fill = \"purple\", bins = 15, alpha = 0.7) +\n  facet_wrap(~ cut(AGE, breaks = seq(20, 80, by = 10))) +  # Group age into bins\n  labs(title = \"Histogram of total drug Y for Different Age Groups\",\n       x = \"Dose\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n#Distribution of Y by HT and WT\n\nggplot(drug_data_final, aes(x = HT, y = Y)) +\n  geom_point(color = \"blue\", alpha = 0.6) +  \n  geom_smooth(method = \"loess\", color = \"red\", se = FALSE) +  \n  labs(title = \"Scatter Plot of Total Drug (Y) by Height\",\n       x = \"Height\",\n       y = \"Total Drug (Y)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#WT\n\nggplot(drug_data_final, aes(x = WT, y = Y)) +\n  geom_point(color = \"blue\", alpha = 0.6) +  \n  geom_smooth(method = \"loess\", color = \"red\", se = FALSE) +  \n  labs(title = \"Scatter Plot of Total Drug (Y) by Height\",\n       x = \"Height\",\n       y = \"Total Drug (Y)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#Distribution of Y by Race\n\nggplot(drug_data_final, aes(x = factor(RACE), y = Y)) +\n  stat_summary(fun = mean, geom = \"col\", fill = \"orange\") +\n  labs(title = \"Average Total Drug (Y) by Race\",\n       x = \"Race\",\n       y = \"Mean Total Drug (Y)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n#Y by Sex\n\nggplot(drug_data_final, aes(x = factor(SEX), y = Y)) +\n  stat_summary(fun = mean, geom = \"col\", fill = \"pink\") +\n  labs(title = \"Average Total Drug (Y) by Sex\",\n       x = \"Race\",\n       y = \"Mean Total Drug (Y)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\nNow I will use tidymodels to fit a linear model to Y by Dose\n\nFirst i will use parsnips package to specify the model that I want to use. Since there are continouos variables this will be a linear model\n\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n#From here I can estimate the model using the fit function. I will be looking at Y ~ Dose x ID\n\nlm_fit &lt;-\n  lm_model %&gt;%\n  fit(Y ~ DOSE, data = drug_data_final)\nlm_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ DOSE, data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n     323.06        58.21  \n\n\n#I can use tidy function to describe my model\n\ntidy(lm_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    323.     199.        1.62 1.07e- 1\n2 DOSE            58.2      5.19     11.2  2.69e-20\n\n\n\n# Define a linear regression model\nlm_model &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\")\n\n# Create a recipe (preprocessing steps, if any)\nlm_recipe &lt;- recipe(Y ~ DOSE, data = drug_data_final)\n\n# Create a workflow\nlm_workflow &lt;- workflow() %&gt;%\n  add_model(lm_model) %&gt;%\n  add_recipe(lm_recipe)\n\n# Fit the model\nlm_fit &lt;- lm_workflow %&gt;% fit(data = drug_data_final)\n\n\n# Make predictions on test data\npredictions &lt;- predict(lm_fit, new_data = drug_data_final) %&gt;%\n  bind_cols(drug_data_final)  # Combine predictions with actual values\n\n# Compute performance metrics\nmetrics &lt;- predictions %&gt;%\n  metrics(truth = Y, estimate = .pred)\n\n# Print results\nmetrics\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     666.   \n2 rsq     standard       0.516\n3 mae     standard     517.   \n\n\n#Now I will repeat the process but I will fit the model to outcome Y using all predictors\n\nlm_fit2 &lt;-\n  lm_model %&gt;%\n  fit(Y ~ DOSE * RATE * AGE * SEX * SEX * RACE * HT * WT, data=drug_data_final)\nlm_fit2\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ DOSE * RATE * AGE * SEX * SEX * RACE * \n    HT * WT, data = data)\n\nCoefficients:\n                    (Intercept)                             DOSE  \n                     -1.670e+06                        1.099e+05  \n                           RATE                              AGE  \n                     -1.351e+01                        6.996e+04  \n                           SEX2                            RACE2  \n                     -1.662e+05                       -7.355e+05  \n                          RACE7                           RACE88  \n                     -4.434e+03                       -2.067e+04  \n                             HT                               WT  \n                      1.011e+06                        1.857e+04  \n                      DOSE:RATE                         DOSE:AGE  \n                     -2.750e+02                       -4.532e+03  \n                       RATE:AGE                        DOSE:SEX2  \n                             NA                       -9.367e+02  \n                      RATE:SEX2                         AGE:SEX2  \n                             NA                        6.684e+03  \n                     DOSE:RACE2                       DOSE:RACE7  \n                      3.011e+04                        1.751e+02  \n                    DOSE:RACE88                       RATE:RACE2  \n                      1.209e+03                               NA  \n                     RATE:RACE7                      RATE:RACE88  \n                             NA                               NA  \n                      AGE:RACE2                        AGE:RACE7  \n                      2.598e+04                               NA  \n                     AGE:RACE88                       SEX2:RACE2  \n                     -1.281e+02                       -6.698e+03  \n                     SEX2:RACE7                      SEX2:RACE88  \n                             NA                        1.310e+03  \n                        DOSE:HT                          RATE:HT  \n                     -6.603e+04                               NA  \n                         AGE:HT                          SEX2:HT  \n                     -4.125e+04                        4.329e+04  \n                       RACE2:HT                         RACE7:HT  \n                      4.195e+05                               NA  \n                      RACE88:HT                          DOSE:WT  \n                      1.476e+04                       -1.230e+03  \n                        RATE:WT                           AGE:WT  \n                             NA                       -8.288e+02  \n                        SEX2:WT                         RACE2:WT  \n                      1.961e+02                        8.861e+03  \n                       RACE7:WT                        RACE88:WT  \n                             NA                       -3.116e+01  \n                          HT:WT                    DOSE:RATE:AGE  \n                     -1.127e+04                        1.111e+01  \n                 DOSE:RATE:SEX2                    DOSE:AGE:SEX2  \n                     -2.638e+00                       -5.239e+01  \n                  RATE:AGE:SEX2                  DOSE:RATE:RACE2  \n                             NA                       -1.732e+00  \n                DOSE:RATE:RACE7                 DOSE:RATE:RACE88  \n                             NA                               NA  \n                 DOSE:AGE:RACE2                   DOSE:AGE:RACE7  \n                     -1.005e+03                               NA  \n                DOSE:AGE:RACE88                   RATE:AGE:RACE2  \n                      3.276e-01                               NA  \n                 RATE:AGE:RACE7                  RATE:AGE:RACE88  \n                             NA                               NA  \n                DOSE:SEX2:RACE2                  DOSE:SEX2:RACE7  \n                             NA                               NA  \n               DOSE:SEX2:RACE88                  RATE:SEX2:RACE2  \n                             NA                               NA  \n                RATE:SEX2:RACE7                 RATE:SEX2:RACE88  \n                             NA                               NA  \n                 AGE:SEX2:RACE2                   AGE:SEX2:RACE7  \n                     -2.170e+02                               NA  \n                AGE:SEX2:RACE88                     DOSE:RATE:HT  \n                             NA                        1.640e+02  \n                    DOSE:AGE:HT                      RATE:AGE:HT  \n                      2.665e+03                               NA  \n                   DOSE:SEX2:HT                     RATE:SEX2:HT  \n                      2.439e+03                               NA  \n                    AGE:SEX2:HT                    DOSE:RACE2:HT  \n                     -2.812e+03                       -1.676e+04  \n                  DOSE:RACE7:HT                   DOSE:RACE88:HT  \n                             NA                       -6.889e+02  \n                  RATE:RACE2:HT                    RATE:RACE7:HT  \n                             NA                               NA  \n                 RATE:RACE88:HT                     AGE:RACE2:HT  \n                             NA                       -1.485e+04  \n                   AGE:RACE7:HT                    AGE:RACE88:HT  \n                             NA                               NA  \n                  SEX2:RACE2:HT                    SEX2:RACE7:HT  \n                      9.375e+03                               NA  \n                 SEX2:RACE88:HT                     DOSE:RATE:WT  \n                             NA                        3.108e+00  \n                    DOSE:AGE:WT                      RATE:AGE:WT  \n                      5.373e+01                               NA  \n                   DOSE:SEX2:WT                     RATE:SEX2:WT  \n                      4.914e+00                               NA  \n                    AGE:SEX2:WT                    DOSE:RACE2:WT  \n                     -8.613e+00                       -3.546e+02  \n                  DOSE:RACE7:WT                   DOSE:RACE88:WT  \n                             NA                               NA  \n                  RATE:RACE2:WT                    RATE:RACE7:WT  \n                             NA                               NA  \n                 RATE:RACE88:WT                     AGE:RACE2:WT  \n                             NA                       -3.048e+02  \n                   AGE:RACE7:WT                    AGE:RACE88:WT  \n                             NA                               NA  \n                  SEX2:RACE2:WT                    SEX2:RACE7:WT  \n                             NA                               NA  \n                 SEX2:RACE88:WT                       DOSE:HT:WT  \n                             NA                        7.414e+02  \n                     RATE:HT:WT                        AGE:HT:WT  \n                             NA                        4.875e+02  \n                     SEX2:HT:WT                      RACE2:HT:WT  \n                             NA                       -5.126e+03  \n                    RACE7:HT:WT                     RACE88:HT:WT  \n                             NA                               NA  \n             DOSE:RATE:AGE:SEX2              DOSE:RATE:AGE:RACE2  \n                             NA                        5.726e-02  \n            DOSE:RATE:AGE:RACE7             DOSE:RATE:AGE:RACE88  \n                             NA                               NA  \n           DOSE:RATE:SEX2:RACE2             DOSE:RATE:SEX2:RACE7  \n                             NA                               NA  \n          DOSE:RATE:SEX2:RACE88              DOSE:AGE:SEX2:RACE2  \n                             NA                               NA  \n            DOSE:AGE:SEX2:RACE7             DOSE:AGE:SEX2:RACE88  \n                             NA                               NA  \n            RATE:AGE:SEX2:RACE2              RATE:AGE:SEX2:RACE7  \n                             NA                               NA  \n           RATE:AGE:SEX2:RACE88                 DOSE:RATE:AGE:HT  \n                             NA                       -6.504e+00  \n              DOSE:RATE:SEX2:HT                 DOSE:AGE:SEX2:HT  \n                             NA                               NA  \n               RATE:AGE:SEX2:HT               DOSE:RATE:RACE2:HT  \n                             NA                               NA  \n             DOSE:RATE:RACE7:HT              DOSE:RATE:RACE88:HT  \n                             NA                               NA  \n              DOSE:AGE:RACE2:HT                DOSE:AGE:RACE7:HT  \n                      5.621e+02                               NA  \n             DOSE:AGE:RACE88:HT                RATE:AGE:RACE2:HT  \n                             NA                               NA  \n              RATE:AGE:RACE7:HT               RATE:AGE:RACE88:HT  \n                             NA                               NA  \n             DOSE:SEX2:RACE2:HT               DOSE:SEX2:RACE7:HT  \n                             NA                               NA  \n            DOSE:SEX2:RACE88:HT               RATE:SEX2:RACE2:HT  \n                             NA                               NA  \n             RATE:SEX2:RACE7:HT              RATE:SEX2:RACE88:HT  \n                             NA                               NA  \n              AGE:SEX2:RACE2:HT                AGE:SEX2:RACE7:HT  \n                             NA                               NA  \n             AGE:SEX2:RACE88:HT                 DOSE:RATE:AGE:WT  \n                             NA                       -1.318e-01  \n              DOSE:RATE:SEX2:WT                 DOSE:AGE:SEX2:WT  \n                             NA                               NA  \n               RATE:AGE:SEX2:WT               DOSE:RATE:RACE2:WT  \n                             NA                               NA  \n             DOSE:RATE:RACE7:WT              DOSE:RATE:RACE88:WT  \n                             NA                               NA  \n              DOSE:AGE:RACE2:WT                DOSE:AGE:RACE7:WT  \n                      1.161e+01                               NA  \n             DOSE:AGE:RACE88:WT                RATE:AGE:RACE2:WT  \n                             NA                               NA  \n              RATE:AGE:RACE7:WT               RATE:AGE:RACE88:WT  \n                             NA                               NA  \n             DOSE:SEX2:RACE2:WT               DOSE:SEX2:RACE7:WT  \n                             NA                               NA  \n            DOSE:SEX2:RACE88:WT               RATE:SEX2:RACE2:WT  \n                             NA                               NA  \n             RATE:SEX2:RACE7:WT              RATE:SEX2:RACE88:WT  \n                             NA                               NA  \n              AGE:SEX2:RACE2:WT                AGE:SEX2:RACE7:WT  \n                             NA                               NA  \n             AGE:SEX2:RACE88:WT                  DOSE:RATE:HT:WT  \n                             NA                       -1.857e+00  \n                 DOSE:AGE:HT:WT                   RATE:AGE:HT:WT  \n                     -3.153e+01                               NA  \n                DOSE:SEX2:HT:WT                  RATE:SEX2:HT:WT  \n                             NA                               NA  \n                 AGE:SEX2:HT:WT                 DOSE:RACE2:HT:WT  \n                             NA                        2.020e+02  \n               DOSE:RACE7:HT:WT                DOSE:RACE88:HT:WT  \n                             NA                               NA  \n               RATE:RACE2:HT:WT                 RATE:RACE7:HT:WT  \n                             NA                               NA  \n              RATE:RACE88:HT:WT                  AGE:RACE2:HT:WT  \n                             NA                        1.768e+02  \n                AGE:RACE7:HT:WT                 AGE:RACE88:HT:WT  \n                             NA                               NA  \n               SEX2:RACE2:HT:WT                 SEX2:RACE7:HT:WT  \n                             NA                               NA  \n              SEX2:RACE88:HT:WT         DOSE:RATE:AGE:SEX2:RACE2  \n                             NA                               NA  \n       DOSE:RATE:AGE:SEX2:RACE7        DOSE:RATE:AGE:SEX2:RACE88  \n                             NA                               NA  \n          DOSE:RATE:AGE:SEX2:HT           DOSE:RATE:AGE:RACE2:HT  \n                             NA                               NA  \n         DOSE:RATE:AGE:RACE7:HT          DOSE:RATE:AGE:RACE88:HT  \n                             NA                               NA  \n        DOSE:RATE:SEX2:RACE2:HT          DOSE:RATE:SEX2:RACE7:HT  \n                             NA                               NA  \n       DOSE:RATE:SEX2:RACE88:HT           DOSE:AGE:SEX2:RACE2:HT  \n                             NA                               NA  \n         DOSE:AGE:SEX2:RACE7:HT          DOSE:AGE:SEX2:RACE88:HT  \n                             NA                               NA  \n         RATE:AGE:SEX2:RACE2:HT           RATE:AGE:SEX2:RACE7:HT  \n                             NA                               NA  \n        RATE:AGE:SEX2:RACE88:HT            DOSE:RATE:AGE:SEX2:WT  \n                             NA                               NA  \n         DOSE:RATE:AGE:RACE2:WT           DOSE:RATE:AGE:RACE7:WT  \n                             NA                               NA  \n        DOSE:RATE:AGE:RACE88:WT          DOSE:RATE:SEX2:RACE2:WT  \n                             NA                               NA  \n        DOSE:RATE:SEX2:RACE7:WT         DOSE:RATE:SEX2:RACE88:WT  \n                             NA                               NA  \n         DOSE:AGE:SEX2:RACE2:WT           DOSE:AGE:SEX2:RACE7:WT  \n                             NA                               NA  \n        DOSE:AGE:SEX2:RACE88:WT           RATE:AGE:SEX2:RACE2:WT  \n                             NA                               NA  \n         RATE:AGE:SEX2:RACE7:WT          RATE:AGE:SEX2:RACE88:WT  \n                             NA                               NA  \n            DOSE:RATE:AGE:HT:WT             DOSE:RATE:SEX2:HT:WT  \n                      7.703e-02                               NA  \n            DOSE:AGE:SEX2:HT:WT              RATE:AGE:SEX2:HT:WT  \n                             NA                               NA  \n          DOSE:RATE:RACE2:HT:WT            DOSE:RATE:RACE7:HT:WT  \n                             NA                               NA  \n         DOSE:RATE:RACE88:HT:WT             DOSE:AGE:RACE2:HT:WT  \n                             NA                       -6.649e+00  \n           DOSE:AGE:RACE7:HT:WT            DOSE:AGE:RACE88:HT:WT  \n                             NA                               NA  \n           RATE:AGE:RACE2:HT:WT             RATE:AGE:RACE7:HT:WT  \n                             NA                               NA  \n          RATE:AGE:RACE88:HT:WT            DOSE:SEX2:RACE2:HT:WT  \n                             NA                               NA  \n          DOSE:SEX2:RACE7:HT:WT           DOSE:SEX2:RACE88:HT:WT  \n                             NA                               NA  \n          RATE:SEX2:RACE2:HT:WT            RATE:SEX2:RACE7:HT:WT  \n                             NA                               NA  \n         RATE:SEX2:RACE88:HT:WT             AGE:SEX2:RACE2:HT:WT  \n                             NA                               NA  \n           AGE:SEX2:RACE7:HT:WT            AGE:SEX2:RACE88:HT:WT  \n                             NA                               NA  \n    DOSE:RATE:AGE:SEX2:RACE2:HT      DOSE:RATE:AGE:SEX2:RACE7:HT  \n                             NA                               NA  \n   DOSE:RATE:AGE:SEX2:RACE88:HT      DOSE:RATE:AGE:SEX2:RACE2:WT  \n                             NA                               NA  \n    DOSE:RATE:AGE:SEX2:RACE7:WT     DOSE:RATE:AGE:SEX2:RACE88:WT  \n                             NA                               NA  \n       DOSE:RATE:AGE:SEX2:HT:WT        DOSE:RATE:AGE:RACE2:HT:WT  \n                             NA                               NA  \n      DOSE:RATE:AGE:RACE7:HT:WT       DOSE:RATE:AGE:RACE88:HT:WT  \n                             NA                               NA  \n     DOSE:RATE:SEX2:RACE2:HT:WT       DOSE:RATE:SEX2:RACE7:HT:WT  \n                             NA                               NA  \n    DOSE:RATE:SEX2:RACE88:HT:WT        DOSE:AGE:SEX2:RACE2:HT:WT  \n                             NA                               NA  \n      DOSE:AGE:SEX2:RACE7:HT:WT       DOSE:AGE:SEX2:RACE88:HT:WT  \n                             NA                               NA  \n      RATE:AGE:SEX2:RACE2:HT:WT        RATE:AGE:SEX2:RACE7:HT:WT  \n                             NA                               NA  \n     RATE:AGE:SEX2:RACE88:HT:WT   DOSE:RATE:AGE:SEX2:RACE2:HT:WT  \n                             NA                               NA  \n DOSE:RATE:AGE:SEX2:RACE7:HT:WT  DOSE:RATE:AGE:SEX2:RACE88:HT:WT  \n                             NA                               NA  \n\n\n\ntidy(lm_fit2)\n\n# A tibble: 256 × 5\n   term          estimate std.error statistic p.value\n   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) -1670413.   8461763.   -0.197    0.844\n 2 DOSE          109945.    506900.    0.217    0.829\n 3 RATE             -13.5      241.   -0.0561   0.955\n 4 AGE            69956.    217129.    0.322    0.749\n 5 SEX2         -166230.    346885.   -0.479    0.634\n 6 RACE2        -735484.    779340.   -0.944    0.350\n 7 RACE7          -4434.     12643.   -0.351    0.727\n 8 RACE88        -20669.     67099.   -0.308    0.759\n 9 HT           1010639.   5013084.    0.202    0.841\n10 WT             18569.     99388.    0.187    0.853\n# ℹ 246 more rows\n\n\n#R-squared and RMSE\n\n# Define a linear regression model\nlm_model &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\")\n\n# Create a recipe (preprocessing steps, if any)\nlm_recipe2 &lt;- recipe(Y ~ DOSE + RATE + AGE + SEX + RACE + HT + WT, data = drug_data_final)\n\n# Create a workflow\nlm_workflow2 &lt;- workflow() %&gt;%\n  add_model(lm_model) %&gt;%\n  add_recipe(lm_recipe)\n\n# Fit the model\nlm_fit2 &lt;- lm_workflow %&gt;% fit(data = drug_data_final)\n\n\n# Make predictions on test data\npredictions2 &lt;- predict(lm_fit2, new_data = drug_data_final) %&gt;%\n  bind_cols(drug_data_final) \n\n# Compute performance metrics\nmetrics2 &lt;- predictions2 %&gt;%\n  metrics(truth = Y, estimate = .pred)\n\n# Print results\nmetrics2\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     666.   \n2 rsq     standard       0.516\n3 mae     standard     517.   \n\n\n#Logistic model requires a categorical variable, I dont know which is male or female but i will make sure this variable categorical\n\ndrug_data_final$SEX &lt;- as.factor(drug_data_final$SEX)\n\n####Now I will repeat the process with a logistic model. starting with the parsnips package to specify my model\n\nlog_model &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\n#Fitting the model\n\nlog_fit &lt;-\n  log_model %&gt;%\n  fit(SEX ~ DOSE , data = drug_data_final)\nlog_fit\n\nparsnip model object\n\n\nCall:  stats::glm(formula = SEX ~ DOSE, family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n   -0.76482     -0.03175  \n\nDegrees of Freedom: 119 Total (i.e. Null);  118 Residual\nNull Deviance:      94.24 \nResidual Deviance: 92.43    AIC: 96.43\n\n\n#Fitting the model the long way?\n\nlogistic_mod &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;%  \n  set_mode(\"classification\")  \n\n#Creating a recipe\ndata_recipe &lt;- recipe(SEX ~ DOSE, data=drug_data_final) \n#Creating a workflow\nlogistic_workflow &lt;- workflow() %&gt;%\n  add_model(logistic_mod) %&gt;%\n  add_recipe(data_recipe)\n#fitting the model\nlogistic_fit &lt;- logistic_workflow %&gt;%\n  fit(data = drug_data_final)\n#checking to see if it worked \ntidy(logistic_fit)  # View model coefficients\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -0.765     0.854     -0.896   0.370\n2 DOSE         -0.0318    0.0243    -1.31    0.192\n\nglance(logistic_fit)  # View overall model performance\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1          94.2     119  -46.2  96.4  102.     92.4         118   120\n\n\n#Computing accuracy\n\nset.seed(1)\ndata_split &lt;- initial_split(drug_data_final, prop = 0.8, strata = SEX)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n#adding the recipe and workflow\n#Creating a recipe\ndata_recipe &lt;- recipe(SEX ~ DOSE, data=drug_data_final) \n#Creating a workflow\nlogistic_workflow &lt;- workflow() %&gt;%\n  add_model(logistic_mod) %&gt;%\n  add_recipe(data_recipe)\n#fit the model to training data\nlogistic_fit &lt;- logistic_workflow %&gt;%\n  fit(data = train_data)\n#Making predictions on data\ntest_predictions &lt;- logistic_fit %&gt;%\n  predict(new_data = test_data, type = \"prob\") %&gt;%  # Get probabilities\n  bind_cols(predict(logistic_fit, new_data = test_data)) %&gt;%  # Get class predictions\n  bind_cols(test_data)  # Add actual values\n#computing accuracy\naccuracy_result &lt;- test_predictions %&gt;%\n  metrics(truth = SEX, estimate = .pred_class) %&gt;%\n  filter(.metric == \"accuracy\")\n\nprint(accuracy_result)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary          0.84\n\nroc_auc_result &lt;- test_predictions %&gt;%\n  roc_auc(truth = SEX, 1)  # Adjust `.pred_Male` based on levels in SEX\n\nprint(roc_auc_result)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.786\n\n\n#Fitting to all model\n\nlogistic_mod2 &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;%  \n  set_mode(\"classification\")  \nlogistic_workflow2 &lt;- workflow() %&gt;%\n  add_model(logistic_mod2) %&gt;%\n  add_formula(SEX ~ DOSE * RATE * AGE * Y * RACE * HT * WT)\nlogisitic_fit2 &lt;- logistic_workflow2 %&gt;%\n  fit(data = drug_data_final)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nlog_fit2 &lt;-\n  log_model %&gt;%\n  fit(SEX ~ DOSE * RATE * AGE * Y * RACE * HT * WT, data = drug_data_final)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nlog_fit2\n\nparsnip model object\n\n\nCall:  stats::glm(formula = SEX ~ DOSE * RATE * AGE * Y * RACE * HT * \n    WT, family = stats::binomial, data = data)\n\nCoefficients:\n                 (Intercept)                          DOSE  \n                  -6.135e+29                     7.500e+28  \n                        RATE                           AGE  \n                  -6.365e+27                     1.714e+28  \n                           Y                         RACE2  \n                   2.413e+26                    -6.749e+29  \n                       RACE7                        RACE88  \n                   8.572e+15                     9.761e+17  \n                          HT                            WT  \n                   3.252e+29                     6.360e+26  \n                   DOSE:RATE                      DOSE:AGE  \n                  -8.179e+25                    -1.028e+27  \n                    RATE:AGE                        DOSE:Y  \n                          NA                    -1.448e+25  \n                      RATE:Y                         AGE:Y  \n                          NA                    -6.718e+24  \n                  DOSE:RACE2                    DOSE:RACE7  \n                   3.119e+28                    -5.673e+14  \n                 DOSE:RACE88                    RATE:RACE2  \n                   3.633e+15                            NA  \n                  RATE:RACE7                   RATE:RACE88  \n                          NA                            NA  \n                   AGE:RACE2                     AGE:RACE7  \n                  -4.850e+28                            NA  \n                  AGE:RACE88                       Y:RACE2  \n                  -1.202e+16                     2.105e+26  \n                     Y:RACE7                      Y:RACE88  \n                          NA                    -9.078e+13  \n                     DOSE:HT                       RATE:HT  \n                  -1.951e+28                            NA  \n                      AGE:HT                          Y:HT  \n                  -1.036e+28                    -1.202e+26  \n                    RACE2:HT                      RACE7:HT  \n                   2.564e+29                            NA  \n                   RACE88:HT                       DOSE:WT  \n                  -2.478e+17                    -3.816e+25  \n                     RATE:WT                        AGE:WT  \n                          NA                     8.679e+24  \n                        Y:WT                      RACE2:WT  \n                  -4.193e+23                     4.796e+27  \n                    RACE7:WT                     RACE88:WT  \n                          NA                    -3.453e+15  \n                       HT:WT                 DOSE:RATE:AGE  \n                   3.357e+17                     2.285e+24  \n                 DOSE:RATE:Y                    DOSE:AGE:Y  \n                   3.218e+22                     4.031e+23  \n                  RATE:AGE:Y               DOSE:RATE:RACE2  \n                          NA                    -2.794e+25  \n             DOSE:RATE:RACE7              DOSE:RATE:RACE88  \n                          NA                            NA  \n              DOSE:AGE:RACE2                DOSE:AGE:RACE7  \n                   2.431e+27                            NA  \n             DOSE:AGE:RACE88                RATE:AGE:RACE2  \n                   1.083e+14                            NA  \n              RATE:AGE:RACE7               RATE:AGE:RACE88  \n                          NA                            NA  \n                DOSE:Y:RACE2                  DOSE:Y:RACE7  \n                  -8.421e+24                            NA  \n               DOSE:Y:RACE88                  RATE:Y:RACE2  \n                   1.972e+10                            NA  \n                RATE:Y:RACE7                 RATE:Y:RACE88  \n                          NA                            NA  \n                 AGE:Y:RACE2                   AGE:Y:RACE7  \n                   4.236e+24                            NA  \n                AGE:Y:RACE88                  DOSE:RATE:HT  \n                          NA                     4.336e+25  \n                 DOSE:AGE:HT                   RATE:AGE:HT  \n                   6.216e+26                            NA  \n                   DOSE:Y:HT                     RATE:Y:HT  \n                   7.210e+24                            NA  \n                    AGE:Y:HT                 DOSE:RACE2:HT  \n                   3.901e+24                    -1.026e+28  \n               DOSE:RACE7:HT                DOSE:RACE88:HT  \n                          NA                            NA  \n               RATE:RACE2:HT                 RATE:RACE7:HT  \n                          NA                            NA  \n              RATE:RACE88:HT                  AGE:RACE2:HT  \n                          NA                     1.384e+28  \n                AGE:RACE7:HT                 AGE:RACE88:HT  \n                          NA                            NA  \n                  Y:RACE2:HT                    Y:RACE7:HT  \n                  -1.177e+26                            NA  \n                 Y:RACE88:HT                  DOSE:RATE:WT  \n                          NA                     8.480e+22  \n                 DOSE:AGE:WT                   RATE:AGE:WT  \n                  -5.208e+23                            NA  \n                   DOSE:Y:WT                     RATE:Y:WT  \n                   2.516e+22                            NA  \n                    AGE:Y:WT                 DOSE:RACE2:WT  \n                  -1.683e+13                    -1.918e+26  \n               DOSE:RACE7:WT                DOSE:RACE88:WT  \n                          NA                            NA  \n               RATE:RACE2:WT                 RATE:RACE7:WT  \n                          NA                            NA  \n              RATE:RACE88:WT                  AGE:RACE2:WT  \n                          NA                     3.437e+26  \n                AGE:RACE7:WT                 AGE:RACE88:WT  \n                          NA                            NA  \n                  Y:RACE2:WT                    Y:RACE7:WT  \n                  -2.227e+24                            NA  \n                 Y:RACE88:WT                    DOSE:HT:WT  \n                          NA                    -6.041e+15  \n                  RATE:HT:WT                     AGE:HT:WT  \n                          NA                    -1.318e+16  \n                     Y:HT:WT                   RACE2:HT:WT  \n                  -2.552e+14                    -2.596e+27  \n                 RACE7:HT:WT                  RACE88:HT:WT  \n                          NA                            NA  \n             DOSE:RATE:AGE:Y           DOSE:RATE:AGE:RACE2  \n                  -8.958e+20                    -3.274e+24  \n         DOSE:RATE:AGE:RACE7          DOSE:RATE:AGE:RACE88  \n                          NA                            NA  \n           DOSE:RATE:Y:RACE2             DOSE:RATE:Y:RACE7  \n                          NA                            NA  \n          DOSE:RATE:Y:RACE88              DOSE:AGE:Y:RACE2  \n                          NA                    -1.694e+23  \n            DOSE:AGE:Y:RACE7             DOSE:AGE:Y:RACE88  \n                          NA                            NA  \n            RATE:AGE:Y:RACE2              RATE:AGE:Y:RACE7  \n                          NA                            NA  \n           RATE:AGE:Y:RACE88              DOSE:RATE:AGE:HT  \n                          NA                    -1.381e+24  \n              DOSE:RATE:Y:HT                 DOSE:AGE:Y:HT  \n                  -1.602e+22                    -2.341e+23  \n               RATE:AGE:Y:HT            DOSE:RATE:RACE2:HT  \n                          NA                            NA  \n          DOSE:RATE:RACE7:HT           DOSE:RATE:RACE88:HT  \n                          NA                            NA  \n           DOSE:AGE:RACE2:HT             DOSE:AGE:RACE7:HT  \n                  -5.536e+26                            NA  \n          DOSE:AGE:RACE88:HT             RATE:AGE:RACE2:HT  \n                          NA                            NA  \n           RATE:AGE:RACE7:HT            RATE:AGE:RACE88:HT  \n                          NA                            NA  \n             DOSE:Y:RACE2:HT               DOSE:Y:RACE7:HT  \n                   4.707e+24                            NA  \n            DOSE:Y:RACE88:HT               RATE:Y:RACE2:HT  \n                          NA                            NA  \n             RATE:Y:RACE7:HT              RATE:Y:RACE88:HT  \n                          NA                            NA  \n              AGE:Y:RACE2:HT                AGE:Y:RACE7:HT  \n                  -2.473e+24                            NA  \n             AGE:Y:RACE88:HT              DOSE:RATE:AGE:WT  \n                          NA                     1.157e+21  \n              DOSE:RATE:Y:WT                 DOSE:AGE:Y:WT  \n                  -5.590e+19                     3.259e+11  \n               RATE:AGE:Y:WT            DOSE:RATE:RACE2:WT  \n                          NA                            NA  \n          DOSE:RATE:RACE7:WT           DOSE:RATE:RACE88:WT  \n                          NA                            NA  \n           DOSE:AGE:RACE2:WT             DOSE:AGE:RACE7:WT  \n                  -1.375e+25                            NA  \n          DOSE:AGE:RACE88:WT             RATE:AGE:RACE2:WT  \n                          NA                            NA  \n           RATE:AGE:RACE7:WT            RATE:AGE:RACE88:WT  \n                          NA                            NA  \n             DOSE:Y:RACE2:WT               DOSE:Y:RACE7:WT  \n                   8.909e+22                            NA  \n            DOSE:Y:RACE88:WT               RATE:Y:RACE2:WT  \n                          NA                            NA  \n             RATE:Y:RACE7:WT              RATE:Y:RACE88:WT  \n                          NA                            NA  \n              AGE:Y:RACE2:WT                AGE:Y:RACE7:WT  \n                  -6.826e+22                            NA  \n             AGE:Y:RACE88:WT               DOSE:RATE:HT:WT  \n                          NA                            NA  \n              DOSE:AGE:HT:WT                RATE:AGE:HT:WT  \n                   2.411e+14                            NA  \n                DOSE:Y:HT:WT                  RATE:Y:HT:WT  \n                   4.958e+12                            NA  \n                 AGE:Y:HT:WT              DOSE:RACE2:HT:WT  \n                   9.481e+12                     1.038e+26  \n            DOSE:RACE7:HT:WT             DOSE:RACE88:HT:WT  \n                          NA                            NA  \n            RATE:RACE2:HT:WT              RATE:RACE7:HT:WT  \n                          NA                            NA  \n           RATE:RACE88:HT:WT               AGE:RACE2:HT:WT  \n                          NA                    -7.906e+25  \n             AGE:RACE7:HT:WT              AGE:RACE88:HT:WT  \n                          NA                            NA  \n               Y:RACE2:HT:WT                 Y:RACE7:HT:WT  \n                   2.141e+24                            NA  \n              Y:RACE88:HT:WT         DOSE:RATE:AGE:Y:RACE2  \n                          NA                            NA  \n       DOSE:RATE:AGE:Y:RACE7        DOSE:RATE:AGE:Y:RACE88  \n                          NA                            NA  \n          DOSE:RATE:AGE:Y:HT        DOSE:RATE:AGE:RACE2:HT  \n                   5.201e+20                            NA  \n      DOSE:RATE:AGE:RACE7:HT       DOSE:RATE:AGE:RACE88:HT  \n                          NA                            NA  \n        DOSE:RATE:Y:RACE2:HT          DOSE:RATE:Y:RACE7:HT  \n                          NA                            NA  \n       DOSE:RATE:Y:RACE88:HT           DOSE:AGE:Y:RACE2:HT  \n                          NA                     9.890e+22  \n         DOSE:AGE:Y:RACE7:HT          DOSE:AGE:Y:RACE88:HT  \n                          NA                            NA  \n         RATE:AGE:Y:RACE2:HT           RATE:AGE:Y:RACE7:HT  \n                          NA                            NA  \n        RATE:AGE:Y:RACE88:HT            DOSE:RATE:AGE:Y:WT  \n                          NA                            NA  \n      DOSE:RATE:AGE:RACE2:WT        DOSE:RATE:AGE:RACE7:WT  \n                          NA                            NA  \n     DOSE:RATE:AGE:RACE88:WT          DOSE:RATE:Y:RACE2:WT  \n                          NA                            NA  \n        DOSE:RATE:Y:RACE7:WT         DOSE:RATE:Y:RACE88:WT  \n                          NA                            NA  \n         DOSE:AGE:Y:RACE2:WT           DOSE:AGE:Y:RACE7:WT  \n                   2.730e+21                            NA  \n        DOSE:AGE:Y:RACE88:WT           RATE:AGE:Y:RACE2:WT  \n                          NA                            NA  \n         RATE:AGE:Y:RACE7:WT          RATE:AGE:Y:RACE88:WT  \n                          NA                            NA  \n         DOSE:RATE:AGE:HT:WT             DOSE:RATE:Y:HT:WT  \n                          NA                            NA  \n            DOSE:AGE:Y:HT:WT              RATE:AGE:Y:HT:WT  \n                  -5.797e+21                     9.661e+20  \n       DOSE:RATE:RACE2:HT:WT         DOSE:RATE:RACE7:HT:WT  \n                          NA                            NA  \n      DOSE:RATE:RACE88:HT:WT          DOSE:AGE:RACE2:HT:WT  \n                          NA                            NA  \n        DOSE:AGE:RACE7:HT:WT         DOSE:AGE:RACE88:HT:WT  \n                          NA                            NA  \n        RATE:AGE:RACE2:HT:WT          RATE:AGE:RACE7:HT:WT  \n                   1.293e+23                            NA  \n       RATE:AGE:RACE88:HT:WT            DOSE:Y:RACE2:HT:WT  \n                          NA                            NA  \n          DOSE:Y:RACE7:HT:WT           DOSE:Y:RACE88:HT:WT  \n                          NA                            NA  \n          RATE:Y:RACE2:HT:WT            RATE:Y:RACE7:HT:WT  \n                  -1.731e+22                            NA  \n         RATE:Y:RACE88:HT:WT             AGE:Y:RACE2:HT:WT  \n                          NA                     3.975e+22  \n           AGE:Y:RACE7:HT:WT            AGE:Y:RACE88:HT:WT  \n                          NA                            NA  \n    DOSE:RATE:AGE:Y:RACE2:HT      DOSE:RATE:AGE:Y:RACE7:HT  \n                          NA                            NA  \n   DOSE:RATE:AGE:Y:RACE88:HT      DOSE:RATE:AGE:Y:RACE2:WT  \n                          NA                            NA  \n    DOSE:RATE:AGE:Y:RACE7:WT     DOSE:RATE:AGE:Y:RACE88:WT  \n                          NA                            NA  \n       DOSE:RATE:AGE:Y:HT:WT     DOSE:RATE:AGE:RACE2:HT:WT  \n                          NA                     1.591e+22  \n   DOSE:RATE:AGE:RACE7:HT:WT    DOSE:RATE:AGE:RACE88:HT:WT  \n                          NA                            NA  \n     DOSE:RATE:Y:RACE2:HT:WT       DOSE:RATE:Y:RACE7:HT:WT  \n                   1.215e+20                            NA  \n    DOSE:RATE:Y:RACE88:HT:WT        DOSE:AGE:Y:RACE2:HT:WT  \n                          NA                            NA  \n      DOSE:AGE:Y:RACE7:HT:WT       DOSE:AGE:Y:RACE88:HT:WT  \n                          NA                            NA  \n      RATE:AGE:Y:RACE2:HT:WT        RATE:AGE:Y:RACE7:HT:WT  \n                  -2.650e+20                            NA  \n     RATE:AGE:Y:RACE88:HT:WT   DOSE:RATE:AGE:Y:RACE2:HT:WT  \n                          NA                            NA  \n DOSE:RATE:AGE:Y:RACE7:HT:WT  DOSE:RATE:AGE:Y:RACE88:HT:WT  \n                          NA                            NA  \n\nDegrees of Freedom: 119 Total (i.e. Null);  29 Residual\nNull Deviance:      94.24 \nResidual Deviance: 937.1    AIC: 1119\n\n\n#MODULE 10 STUFF #Removing the RACE variable from the dataset\n#setting seed\n\nrngseed=1234\n\n#Removing variables\n\ndrugdata_new &lt;- drug_data_final %&gt;%\n  select(Y,DOSE,AGE,SEX,WT,HT)\n\n#Random sampling seed\n\nset.seed(rngseed)\n\n#splitting the data 75% training\n\ndata_split &lt;- initial_split(drugdata_new, prop = 3/4)\ntrain_data &lt;- training(data_split)\ntest_data &lt;-testing(data_split)\n\n#Model fitting using dose only as a predictor\n\n# Define a linear regression model\nlm_mod &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\")\n\n# Create the model\nlm_moddose &lt;- lm_mod %&gt;%\n  fit(Y ~ DOSE, data=train_data)\n\n#Model fitting using all as a predictor\n\n# Define a linear regression model\nlm_mod &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\")\n#Create model\nlm_modall &lt;- lm_mod %&gt;%\n  fit(Y ~ DOSE * AGE * SEX * WT * HT, data = train_data)\n\n#Null model\n\n#Null model\nnull_model &lt;- null_model(mode = \"regression\") %&gt;%\n  set_engine(\"parsnip\") %&gt;%\n  fit(Y ~ 1, data= train_data)\n\n#Computing predications lm_mod and lm_modall\n\n#predictions for dose, all, and null\npredictsdose &lt;- predict(lm_moddose, new_data = train_data)\npredictall &lt;-predict(lm_modall, new_data = train_data)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\", : prediction from rank-deficient fit; consider predict(.,\nrankdeficient=\"NA\")\n\npredictnull &lt;- predict(null_model, new_data = train_data)\n\n#Calculating RMSE & r-squared\n\nrmse_dose &lt;- tibble(truth = train_data$Y, predicted = predictsdose$.pred) %&gt;%\n  metrics(truth = truth, estimate = predicted)\nprint(rmse_dose)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     703.   \n2 rsq     standard       0.451\n3 mae     standard     546.   \n\n\n\nrmse_all &lt;- tibble(truth = train_data$Y, predicted = predictall$.pred) %&gt;%\n  metrics(truth = truth, estimate = predicted)\nprint(rmse_all)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     578.   \n2 rsq     standard       0.628\n3 mae     standard     415.   \n\n\n\nrmse_null &lt;- tibble(truth = train_data$Y, predicted = predictnull$.pred) %&gt;%\n  metrics(truth = truth, estimate = predicted)\n\nWarning: A correlation computation is required, but `estimate` is constant and has 0\nstandard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\nprint(rmse_null)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        948.\n2 rsq     standard         NA \n3 mae     standard        765.\n\n\n#Model performance 2\n#Setting a new seed for samplign\n\nset.seed(1234)\n\n#10 fold CV\n\nfolds &lt;- vfold_cv(train_data, v=10)\nfolds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits         id    \n   &lt;list&gt;         &lt;chr&gt; \n 1 &lt;split [81/9]&gt; Fold01\n 2 &lt;split [81/9]&gt; Fold02\n 3 &lt;split [81/9]&gt; Fold03\n 4 &lt;split [81/9]&gt; Fold04\n 5 &lt;split [81/9]&gt; Fold05\n 6 &lt;split [81/9]&gt; Fold06\n 7 &lt;split [81/9]&gt; Fold07\n 8 &lt;split [81/9]&gt; Fold08\n 9 &lt;split [81/9]&gt; Fold09\n10 &lt;split [81/9]&gt; Fold10\n\n\n#Workflow for dose only model\n\ndose_wf &lt;- workflow() %&gt;%\n  add_model(lm_mod) %&gt;%\n  add_formula(Y ~ DOSE)\nset.seed(1111)\ndose_fit_rs &lt;- dose_wf %&gt;%\n  fit_resamples(folds)\n\n#RMSE\n\ncollect_metrics(dose_fit_rs)\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   691.       10 67.5    Preprocessor1_Model1\n2 rsq     standard     0.512    10  0.0592 Preprocessor1_Model1\n\n\n#Workflow for all predictors\n\nall_wf &lt;- workflow() %&gt;%\n  add_model(lm_mod) %&gt;%\n  add_formula(Y ~ DOSE * AGE * SEX * WT * HT)\nset.seed(2222)\nall_fit_rs &lt;- all_wf %&gt;%\n  fit_resamples(folds)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x10\n\n\n\n\n\n\ncollect_metrics(all_fit_rs)\n\n# A tibble: 2 × 6\n  .metric .estimator     mean     n   std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   7599.       10 5695.     Preprocessor1_Model1\n2 rsq     standard      0.347    10    0.0807 Preprocessor1_Model1\n\n\n\n\n\nThis section added by Guozheng Yang\n\n\nModel predictions\nThe code below is to visualize model fitting from the null model, model 1, and model 2.\n\n# Null model: prediction on the train set\npred_model0_gz &lt;- predict(null_model, train_data) %&gt;%\n  bind_cols(train_data[\"Y\"])\ncolnames(pred_model0_gz) &lt;- c(\"pred\", \"Y\")\n\n# Model 1: prediction on the train set\npred_model1_gz &lt;- predict(lm_moddose, train_data) %&gt;%\n  bind_cols(train_data[\"Y\"])\ncolnames(pred_model1_gz) &lt;- c(\"pred\", \"Y\")\n\n# Model 2: prediction on the train set\npred_model2_gz &lt;- predict(lm_modall, train_data) %&gt;%\n  bind_cols(train_data[\"Y\"])\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\", : prediction from rank-deficient fit; consider predict(.,\nrankdeficient=\"NA\")\n\ncolnames(pred_model2_gz) &lt;- c(\"pred\", \"Y\")\n\n# Combine the three data sets and make a plot\ncomb_gz &lt;- rbind(pred_model0_gz, pred_model1_gz, pred_model2_gz) %&gt;%\n  mutate(Model=c(rep(\"Null model\", nrow(pred_model0_gz)), \n                 rep(\"Model 1\", nrow(pred_model1_gz)), \n                 rep(\"Model 2\", nrow(pred_model2_gz)))) %&gt;%\n  ggplot(aes(x=Y, y=pred, fill=Model))+\n  geom_point(size=4, stroke=1, alpha=0.7, shape=21)+\n  geom_abline(intercept=0, slope=1, linetype=\"dashed\", color=\"black\", linewidth=2)+\n  scale_fill_manual(name=\"\", values=c(\"dodgerblue1\",\"palevioletred1\",\"darkorange\"))+\n  scale_x_continuous(limits=c(0, 5000))+\n  scale_y_continuous(limits=c(0, 5000))+\n  labs(x=\"Observed value\", y=\"Predicted value\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=25,color=\"black\",margin=margin(t=15),face=\"bold\"),\n        axis.title.y=element_text(size=25,color=\"black\",margin=margin(r=15),face=\"bold\"),\n        axis.text.x=element_text(color=\"black\",size=20,vjust=0),\n        axis.text.y=element_text(color=\"black\",size=20,hjust=1), \n        legend.position=\"top\",\n        legend.title=element_text(size=20), \n        legend.text=element_text(size=18,vjust=0))\ncomb_gz\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAs shown, the predictions from the null model is a horizontal line, since this model only predicts the mean value of the response. The predictions from model 1 are three horizontal lines, since this model only has DOSE as the predictor which only has three possible values.\nNow I’m making a residual plot for model 2.\n\n# Residual plot for model 2\npred_model2_res_gz &lt;- pred_model2_gz %&gt;%\n  mutate(res=pred-Y) %&gt;%\n  ggplot(aes(x=pred, y=res))+\n  geom_point(size=4, stroke=1, alpha=0.7, shape=21, fill=\"firebrick3\")+\n  geom_abline(intercept=0, slope=0, linetype=\"dashed\", color=\"black\", linewidth=2)+\n  scale_x_continuous(limits=c(0, 5000))+\n  scale_y_continuous(limits=c(-2000, 2000))+\n  labs(x=\"Predicted value\", y=\"Residual\")+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=25,color=\"black\",margin=margin(t=15),face=\"bold\"),\n        axis.title.y=element_text(size=25,color=\"black\",margin=margin(r=15),face=\"bold\"),\n        axis.text.x=element_text(color=\"black\",size=20,vjust=0),\n        axis.text.y=element_text(color=\"black\",size=20,hjust=1))\npred_model2_res_gz\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAs shown, despite some points with relatively small residuals, the points randomly distribute around y=0. This denote the generally good performance of model 2.\n\n\nModel predictions and uncertainty\nNow I want to run a bootstrap to evaluate model 2. The code is shown below.\n\nlibrary(rsample)\n# Set seed\nset.seed(rngseed)\n\n# Bootstrap: 100\ndat_bs &lt;- bootstraps(train_data, times=100)\n\n# Model setting\nmodel2_spec &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n# Fit model 2 on 100 bootstraps and store the predictions\npred_bs &lt;- matrix(NA, nrow=nrow(dat_bs), ncol=nrow(train_data))\nfor (i in seq_len(nrow(dat_bs))){\n  fit_i &lt;- fit(model2_spec, Y ~ ., analysis(dat_bs$splits[[i]]))\n  preds_i &lt;- predict(fit_i, new_data=train_data)$.pred\n  pred_bs[i,] &lt;- preds_i\n}\n\n# Calculate 95% CIs\npreds &lt;- pred_bs |&gt; apply(2, quantile,  c(0.025, 0.5, 0.975)) |&gt;  t()\n\n# Make a plot to show the estimates\nplot_est_gz &lt;- train_data %&gt;%\n  mutate(point_est=pred_model2_gz$pred,\n         median=preds[,2],\n         lower=preds[,1],\n         upper=preds[,3]) %&gt;%\n  ggplot(aes(x=Y))+\n  geom_abline(intercept=0, slope=1, linetype=\"dashed\", color=\"black\", linewidth=1)+\n  geom_point(aes(y=point_est), color=\"gray20\", size=2, shape=16, alpha=.8)+ \n  geom_point(aes(y=median), color=\"firebrick2\", size=2, shape=16, alpha=.8)+\n  geom_errorbar(aes(ymin=lower, ymax=upper), width=.2, color=\"palevioletred2\", alpha=.8)+\n  labs(x=\"Observed value\", y=\"Predicted value\")+\n  scale_x_continuous(limits=c(0, 5000))+\n  scale_y_continuous(limits=c(0, 5000))+\n  theme_bw()+\n  theme(axis.title.x=element_text(size=25,color=\"black\",margin=margin(t=15),face=\"bold\"),\n        axis.title.y=element_text(size=25,color=\"black\",margin=margin(r=15),face=\"bold\"),\n        axis.text.x=element_text(color=\"black\",size=20,vjust=0),\n        axis.text.y=element_text(color=\"black\",size=20,hjust=1))\nplot_est_gz\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAccording to the output, the fitting results from bootstrapping are generally consistent with only fitting model 2 on the train set. Almost all the CIs (red) can cover the point predictions (black). From a big picture, the data points are distributed around the diagonal, which means the predicted values are close to the observed values. To this point, we can conclude the good performance of model 2.\n#Making predictions for the test data using fitted model 2\n\npredictall_2 &lt;-predict(lm_modall, new_data = test_data) \n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\", : prediction from rank-deficient fit; consider predict(.,\nrankdeficient=\"NA\")\n\n\n\npredictalls_2 &lt;- predictall_2 %&gt;%\nbind_cols(test_data$Y) %&gt;%\n  mutate(Model = \"Test\")\n\nNew names:\n• `` -&gt; `...2`\n\ncolnames(predictalls_2) &lt;- c(\"Predicted\", \"Observed\",\"Model\")\n\n\npredictalls &lt;- predictall %&gt;%\n  bind_cols(train_data$Y) %&gt;%\n  mutate(Model = \"Model 2\")\n\nNew names:\n• `` -&gt; `...2`\n\ncolnames(predictalls) &lt;- c(\"Predicted\", \"Observed\", \"Model\")\n\n#Combining into 1 data fram\n\nall_models &lt;- bind_rows(predictalls, predictalls_2)\n\n#Plotting data\n\nggplot(all_models, aes(x=Observed, y=Predicted, color = Model, shape = Model)) +\n  geom_point(alpha = 0.7, size = 3) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"solid\", color = \"black\") +\n  labs(title = \"Observed vs Predicted for Training and Test Data\",\n       x= \"Observed\",\n       y= \"Predicted\") +\n  theme_minimal()+\n  scale_color_manual(values = c(\"red\",\"blue\"))+\n  scale_shape_manual(values = c(16,17)) +\n  coord_cartesian(xlim = c(0,5000), ylim = c(0,5000)) +\ntheme(legend.title=element_blank())\n\n\n\n\n\n\n\n\n#Overall model assessment\nSeeing as the null model was not fit to the line at all, both model 1 and model 2 both perform better than the null.\n#saving the final cleaned data (drug_data_final)\n\nsaveRDS(drug_data_final, \"drug_data.rds\")"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html",
    "href": "ml-models-exercise/ml-models-exercise.html",
    "title": "Moduel 11 Exercise",
    "section": "",
    "text": "#Load in packages\nlibrary(tidyr)\nlibrary(here)\n\nhere() starts at /Users/alexisgonzalez/Desktop/MADA/alexisgonzalez-MADA-portfolio\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(ranger)\n\n#Read the RDS file with the cleaned data\n\nfile.copy(\"/Users/alexisgonzalez/Desktop/MADA/alexisgonzalez-MADA-portfolio/fitting-exercise/drug_data.rds\", \"/Users/alexisgonzalez/Desktop/MADA/alexisgonzalez-MADA-portfolio/ml-models-exercise/drug_data.rds\")\n\n[1] FALSE\n\n\n\ndrug_data &lt;- readRDS(\"/Users/alexisgonzalez/Desktop/MADA/alexisgonzalez-MADA-portfolio/ml-models-exercise/drug_data.rds\")\n\n\nset.seed(1234)\n\n\nunique(drug_data$RACE)\n\n[1] 2  1  88 7 \nLevels: 1 2 7 88\n\n\n\nggplot(drug_data, aes(x = factor(RACE))) +\n  geom_bar(fill = \"pink\") +\n  labs(title = \"Distribution of RACE Variable\", x = \"Race\", y = \"Count\") +\n  theme_minimal() +\n  coord_flip()  # Flip for better readability if many categories\n\n\n\n\n\n\n\n\n#Going based on distribution of race in the general population I will assume 1 = white 2= black 7= asian and 88= hispanic/latino\n#combining categories 7 and 88\n\n# Example: Combine \"Asian\" and \"Pacific Islander\" into \"Asian/Pacific\"\ndrug_data$RACE &lt;- ifelse(drug_data$RACE %in% c(\"7\", \"88\"), \"3\", drug_data$RACE)\n\n#CHecking to see if it worked\n\nggplot(drug_data, aes(x = factor(RACE))) +\n  geom_bar(fill = \"pink\") +\n  labs(title = \"Distribution of RACE Variable\", x = \"Race\", y = \"Count\") +\n  theme_minimal() +\n  coord_flip()  \n\n\n\n\n\n\n\n\n#Pairwise correlation for the continous variables. The continous variables are AGE, WT, HT and Y\n\nnumeric_data &lt;- drug_data[sapply(drug_data, is.numeric)]\ncorrelation_matrix &lt;- cor(numeric_data, use = \"pairwise.complete.obs\")\n\nWarning in cor(numeric_data, use = \"pairwise.complete.obs\"): the standard\ndeviation is zero\n\nprint(correlation_matrix)\n\n              ID           Y CMT EVI2 MDV DV LNDV        AMT TIME       DOSE\nID    1.00000000 -0.27277737  NA   NA  NA NA   NA 0.01136475   NA 0.01136475\nY    -0.27277737  1.00000000  NA   NA  NA NA   NA 0.71808396   NA 0.71808396\nCMT           NA          NA  NA   NA  NA NA   NA         NA   NA         NA\nEVI2          NA          NA  NA   NA  NA NA   NA         NA   NA         NA\nMDV           NA          NA  NA   NA  NA NA   NA         NA   NA         NA\nDV            NA          NA  NA   NA  NA NA   NA         NA   NA         NA\nLNDV          NA          NA  NA   NA  NA NA   NA         NA   NA         NA\nAMT   0.01136475  0.71808396  NA   NA  NA NA   NA 1.00000000   NA 1.00000000\nTIME          NA          NA  NA   NA  NA NA   NA         NA   NA         NA\nDOSE  0.01136475  0.71808396  NA   NA  NA NA   NA 1.00000000   NA 1.00000000\nRATE  0.02604537  0.70657449  NA   NA  NA NA   NA 0.99551768   NA 0.99551768\nAGE  -0.11382680  0.01256372  NA   NA  NA NA   NA 0.07201600   NA 0.07201600\nWT    0.17769460 -0.21287194  NA   NA  NA NA   NA 0.10123185   NA 0.10123185\nHT    0.15785828 -0.15832972  NA   NA  NA NA   NA 0.01877994   NA 0.01877994\n           RATE         AGE          WT          HT\nID   0.02604537 -0.11382680  0.17769460  0.15785828\nY    0.70657449  0.01256372 -0.21287194 -0.15832972\nCMT          NA          NA          NA          NA\nEVI2         NA          NA          NA          NA\nMDV          NA          NA          NA          NA\nDV           NA          NA          NA          NA\nLNDV         NA          NA          NA          NA\nAMT  0.99551768  0.07201600  0.10123185  0.01877994\nTIME         NA          NA          NA          NA\nDOSE 0.99551768  0.07201600  0.10123185  0.01877994\nRATE 1.00000000  0.06232124  0.09172394  0.01741403\nAGE  0.06232124  1.00000000  0.11967399 -0.35185806\nWT   0.09172394  0.11967399  1.00000000  0.59975050\nHT   0.01741403 -0.35185806  0.59975050  1.00000000\n\n\n#Feature engineering\n\n#The formula for bmi is kg/m^2. The height variable is clearly in meters and the weight would be really small if they were in lbs so I will assume it is in kgs\ndrug_bmi &lt;- drug_data %&gt;%\n  mutate(BMI = WT/(HT^2))\n\n#First fit\n\nset.seed(1234)\n\n#recipe for fitting y to all\n\nrecipe1 &lt;- recipe(Y ~ ., data = drug_bmi) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_impute_median(all_numeric_predictors())\n\n#defining the model\n\nmodel1 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n#Workflow\n\nworkflow1 &lt;- workflow() %&gt;%\n  add_model(model1) %&gt;%\n  add_recipe(recipe1)\n\n#fit the model to all of the data\n\nfit1 &lt;- workflow1 %&gt;%\n  fit(data = drug_bmi)\n\n#make predictions\n\npredictions1 &lt;- fit1 %&gt;%\n  predict(new_data = drug_bmi) %&gt;%\n  bind_cols(drug_bmi)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\", : prediction from rank-deficient fit; consider predict(.,\nrankdeficient=\"NA\")\n\n\n#metrics\n\nmetrics1 &lt;- predictions1 %&gt;%\n  metrics(truth = Y, estimate = .pred)\nprint(metrics1)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     527.   \n2 rsq     standard       0.698\n3 mae     standard     404.   \n\n\n#note: the RMSE for the null model was 969.5\n#LASSO #defining the model\n\nlasso_model &lt;- linear_reg(penalty = 0.1 , mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n#workflow\n\nworkflow2 &lt;- workflow() %&gt;%\n  add_model(lasso_model) %&gt;%\n  add_recipe(recipe1) #I can use the same recipe as the last model\n\n#fit the data\n\nlasso_fit &lt;- workflow2 %&gt;%\n  fit(data = drug_bmi)\n\n#take out features\n\ntidy(lasso_fit$fit$fit)\n\n# A tibble: 12 × 3\n   term         estimate penalty\n   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)  2476.        0.1\n 2 ID           -213.        0.1\n 3 AMT          1351.        0.1\n 4 DOSE            2.04      0.1\n 5 RATE         -653.        0.1\n 6 AGE             0.891     0.1\n 7 WT           1756.        0.1\n 8 HT          -1362.        0.1\n 9 BMI         -1624.        0.1\n10 SEX_X2       -388.        0.1\n11 RACE_X2       109.        0.1\n12 RACE_X3      -142.        0.1\n\n\n#make predictions on lasso fit\n\npredictions2 &lt;- lasso_fit %&gt;%\n  predict(new_data = drug_bmi) %&gt;%\n  bind_cols(drug_bmi)\n\n#evaluation\n\nmetrics2 &lt;- predictions2 %&gt;%\n  metrics( truth = Y, estimate = .pred)\nprint(metrics2)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     527.   \n2 rsq     standard       0.698\n3 mae     standard     405.   \n\n\n#The lasso and linear results are almost identical. This means that LASSO isn’t doing much feature selection so the variables that ae being used are relevant.\n#Random Forest model\n\nset.seed(123)\nrngseed &lt;- 123\n\n#define the model\n\nrandom_model &lt;- rand_forest(trees = 150, min_n = 5) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"ranger\", seed = rngseed) \n\n#create workflow\n\nworkflow3 &lt;-workflow() %&gt;%\n  add_model(random_model) %&gt;%\n  add_recipe(recipe1)\n\n#fit the model\n\nrandom_fit &lt;- workflow3 %&gt;%\n  fit(data = drug_bmi)\n\n#make predictions\n\npredictions3  &lt;- random_fit %&gt;%\n  predict(new_data = drug_bmi) %&gt;%\n  bind_cols(drug_bmi)\n\n#evaluation\n\nmetrics3 &lt;- predictions3 %&gt;%\n  metrics( truth = Y, estimate = .pred)\nprint(metrics3)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     307.   \n2 rsq     standard       0.912\n3 mae     standard     228.   \n\n\n#The RMSE and the r-squared values are different from the linear and lasso models. The RMSE is lower while the r-squared value is higher. The dataset may be a little too complex and non-linear for linear regression so the random forest handles it better.\n#Now i will plot the observed vs predicted values for each model.\n\n#combine the prediction datasets\n\n# identify each dataset\npredictions1 &lt;- predictions1 %&gt;% mutate(model = \"Linear\")\npredictions2 &lt;- predictions2 %&gt;% mutate(model = \"LASSO\")\npredictions3 &lt;- predictions3 %&gt;% mutate(model = \"Random Forest\")\n\nall_predictions &lt;- bind_rows(predictions1, predictions2, predictions3)\n\n\nggplot(all_predictions, aes(x = Y, y = .pred, color = model, shape = model)) +\n  geom_point(alpha = 0.6, size = 3) +\n  scale_shape_manual(values = c(\"Linear\" = 15, \"LASSO\" = 17, \"Random Forest\" = 16)) +  \n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\", size = 1) +\n  labs(title = \"Observed vs Predicted Values\",\n       x = \"Observed Values\",\n       y = \"Predicted Values\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n#the linear points and lasso points are nearly identical while random forest plots lie closer to the 45 degree line.\n#Model tuning #LASSO\n\nset.seed(1234)\n\n#tuning the LASSO model with a tune grid\n\nlasso_model2 &lt;- linear_reg(penalty = tune() , mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n#Creating a workflow\n\nlasso_wf &lt;-workflow() %&gt;%\n  add_model(lasso_model2) %&gt;%\n  add_recipe(recipe1)\n\n#Creating a grid\n\nlasso_grid &lt;-tibble(penalty = 10^seq(-5, 2, length.out = 50))\nlasso_grid\n\n# A tibble: 50 × 1\n     penalty\n       &lt;dbl&gt;\n 1 0.00001  \n 2 0.0000139\n 3 0.0000193\n 4 0.0000268\n 5 0.0000373\n 6 0.0000518\n 7 0.0000720\n 8 0.0001   \n 9 0.000139 \n10 0.000193 \n# ℹ 40 more rows\n\n\n#resampling\n\napparent_lasso &lt;- apparent(drug_bmi)\napparent_lasso\n\n# Apparent sampling \n# A tibble: 1 × 2\n  splits            id      \n  &lt;list&gt;            &lt;chr&gt;   \n1 &lt;split [120/120]&gt; Apparent\n\n\n#Tune the model\n\nlasso_tuned &lt;- tune_grid(\n  lasso_wf ,\n  resamples = apparent_lasso,\n  grid = lasso_grid\n)\n\n#metrics\n\nlasso_tuned %&gt;%\n  collect_metrics()\n\n# A tibble: 50 × 7\n     penalty .metric .estimator  mean     n std_err .config              \n       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1 0.00001   &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model01\n 2 0.0000139 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model02\n 3 0.0000193 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model03\n 4 0.0000268 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model04\n 5 0.0000373 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model05\n 6 0.0000518 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model06\n 7 0.0000720 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model07\n 8 0.0001    &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model08\n 9 0.000139  &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model09\n10 0.000193  &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model10\n# ℹ 40 more rows\n\n\n#Not recieving any metrics. This is due to apparent sampling. I retried with bootstrap resampling and recieved metrics.\n#RANDOM FOREST #Set tuning parameters\n\ntune_spec &lt;- rand_forest(\n  mtry = tune(),\n  trees = 300,\n  min_n = tune()\n)%&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"ranger\")\n\n#Create a workflow\n\ntune_wf &lt;- workflow()%&gt;%\n  add_recipe(recipe1) %&gt;%\n  add_model(tune_spec)\n\n#Setting up grid\n\nrt_grid &lt;- grid_regular(\n  mtry(range =c(1,7)),\n  min_n(range = c(1,21)),\n  levels = c(7,7)\n)\n\n#set resamples\n\napparent_rt &lt;-apparent(drug_bmi)\n\n#Tune the model\n\ntune_rt &lt;- tune_grid(\n  tune_wf,\n  resamples = apparent_rt,\n  grid = rt_grid\n)\n\n#View results\n\ntune_rt %&gt;%\n  collect_metrics()\n\n# A tibble: 49 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     1     1 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model01\n 2     2     1 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model02\n 3     3     1 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model03\n 4     4     1 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model04\n 5     5     1 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model05\n 6     6     1 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model06\n 7     7     1 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model07\n 8     1     4 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model08\n 9     2     4 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model09\n10     3     4 &lt;NA&gt;    &lt;NA&gt;          NA    NA      NA Preprocessor1_Model10\n# ℹ 39 more rows\n\n\n#TUNING WITH CV\n#LASSO\n\nset.seed(123)\n\n\nlasso_modelcv &lt;- linear_reg(penalty = tune() , mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n#Creating a workflow\n\nlasso_cv &lt;-workflow() %&gt;%\n  add_model(lasso_modelcv) %&gt;%\n  add_recipe(recipe1)\n\n#Creating a grid\n\nlasso_gridcv &lt;-tibble(penalty = 10^seq(-5, 2, length.out = 50))\nlasso_gridcv\n\n# A tibble: 50 × 1\n     penalty\n       &lt;dbl&gt;\n 1 0.00001  \n 2 0.0000139\n 3 0.0000193\n 4 0.0000268\n 5 0.0000373\n 6 0.0000518\n 7 0.0000720\n 8 0.0001   \n 9 0.000139 \n10 0.000193 \n# ℹ 40 more rows\n\n\n#resampling with cv\n\ncv_data &lt;-vfold_cv(drug_bmi, v=5, repeats =5)\n\n#Tune the model\n\nlasso_tunedcv &lt;- tune_grid(\n  lasso_cv ,\n  resamples = cv_data,\n  grid = lasso_gridcv\n)\n\n#metrics\n\nlasso_tunedcv %&gt;%\n  collect_metrics()\n\n# A tibble: 100 × 7\n     penalty .metric .estimator    mean     n std_err .config              \n       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1 0.00001   rmse    standard   567.       25 17.1    Preprocessor1_Model01\n 2 0.00001   rsq     standard     0.651    25  0.0207 Preprocessor1_Model01\n 3 0.0000139 rmse    standard   567.       25 17.1    Preprocessor1_Model02\n 4 0.0000139 rsq     standard     0.651    25  0.0207 Preprocessor1_Model02\n 5 0.0000193 rmse    standard   567.       25 17.1    Preprocessor1_Model03\n 6 0.0000193 rsq     standard     0.651    25  0.0207 Preprocessor1_Model03\n 7 0.0000268 rmse    standard   567.       25 17.1    Preprocessor1_Model04\n 8 0.0000268 rsq     standard     0.651    25  0.0207 Preprocessor1_Model04\n 9 0.0000373 rmse    standard   567.       25 17.1    Preprocessor1_Model05\n10 0.0000373 rsq     standard     0.651    25  0.0207 Preprocessor1_Model05\n# ℹ 90 more rows\n\n\n\nautoplot(lasso_tunedcv)\n\n\n\n\n\n\n\n\n#Random forest using cv resampling\n\ntune_speccv &lt;- rand_forest(\n  mtry = tune(),\n  trees = 300,\n  min_n = tune()\n)%&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"ranger\")\n\n#Create a workflow\n\ntune_rtcv &lt;- workflow()%&gt;%\n  add_recipe(recipe1) %&gt;%\n  add_model(tune_speccv)\n\n#Setting up grid\n\nrt_gridcv&lt;- grid_regular(\n  mtry(range =c(1,7)),\n  min_n(range = c(1,21)),\n  levels = c(7,7)\n)\n\n#resampling with cv\n\ncv_data &lt;-vfold_cv(drug_bmi, v=5, repeats =5)\n\n#Tune the model\n\ntune_cv &lt;- tune_grid(\n  tune_rtcv,\n  resamples = cv_data,\n  grid = rt_gridcv\n)\n\n#View results\n\nautoplot(tune_cv)\n\n\n\n\n\n\n\n\n\ntune_cv %&gt;%\n  collect_metrics()\n\n# A tibble: 98 × 8\n    mtry min_n .metric .estimator    mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     1     1 rmse    standard   647.       25 20.7    Preprocessor1_Model01\n 2     1     1 rsq     standard     0.601    25  0.0123 Preprocessor1_Model01\n 3     2     1 rmse    standard   612.       25 19.0    Preprocessor1_Model02\n 4     2     1 rsq     standard     0.601    25  0.0125 Preprocessor1_Model02\n 5     3     1 rmse    standard   612.       25 18.3    Preprocessor1_Model03\n 6     3     1 rsq     standard     0.597    25  0.0134 Preprocessor1_Model03\n 7     4     1 rmse    standard   608.       25 18.1    Preprocessor1_Model04\n 8     4     1 rsq     standard     0.600    25  0.0141 Preprocessor1_Model04\n 9     5     1 rmse    standard   611.       25 17.7    Preprocessor1_Model05\n10     5     1 rsq     standard     0.596    25  0.0141 Preprocessor1_Model05\n# ℹ 88 more rows"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "Hope Grismer contributed to this excercise\nThe structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nHair Color\n0\n1\n3\n6\n0\n3\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.66667\n15.976545\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.11111\n21.245261\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nThumb Length\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n47.44444\n8.353309\n35\n42\n47\n50\n62\n▅▅▇▁▅"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n248.784981\n13.224621\n18.8122576\n0.0000078\n\n\nHair ColorBrown\n-12.962131\n5.296530\n-2.4472876\n0.0581300\n\n\nHair ColorRed\n3.152118\n6.389235\n0.4933482\n0.6426696\n\n\nThumb Length\n-1.614891\n0.270587\n-5.9681015\n0.0018907"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#box-plot-height-x-hair-color",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#box-plot-height-x-hair-color",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.1 6.1 Box plot Height x Hair Color",
    "text": "6.1 6.1 Box plot Height x Hair Color\nThis box plot visualizes the distribution of height across 3 different hair colors. The range of heights in the Brown hair category is wider than that of Blonde and Red hair. This would show that while Brown haired individuals are more common and exist in a range of heights, they tend to be shorter than Red haired and Blond haired individuals.\n\np5 &lt;- mydata %&gt;% ggplot(aes(x=`Hair Color`, y=Height)) + \n  geom_boxplot(color=\"firebrick3\") \nplot(p5)\n\n\n\n\n\n\n\nfigure_file=here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-hair-boxplot.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#scatter-plot-weight-x-thumb-length",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#scatter-plot-weight-x-thumb-length",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.2 6.2 Scatter plot Weight x Thumb Length",
    "text": "6.2 6.2 Scatter plot Weight x Thumb Length\nThis scatter plot shows the relationship between weight and thumb length. A slight negative trend can be seen, as weight increases, thumb length decreases.\n\np6 &lt;- mydata %&gt;% ggplot(aes(x=Weight, y=`Thumb Length`)) + \n  geom_point(size=2, color=\"firebrick3\")\nplot(p6)\n\n\n\n\n\n\n\nfigure_file=here(\"starter-analysis-exercise\",\"results\",\"figures\",\"thumbsize-weight-scatterplot.png\")\nggsave(filename = figure_file, plot=p6)\n\nSaving 7 x 5 in image\n\n\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Alexis Gonzalez Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Alexis Gonzalez Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/alexisgonzalez/Desktop/MADA/alexisgonzalez-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/alexisgonzalez/Desktop/MADA/alexisgonzalez-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n4 Hair color      Color of natural unaltered hair       Brown,Blonde,Red      \n5 Thumb length    length in cm                          numeric value &gt;0 or NA\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height         &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\",…\n$ Weight         &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, …\n$ Gender         &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\",…\n$ `Hair Color`   &lt;chr&gt; \"Brown\", \"Brown\", \"Blonde\", \"Red\", \"Blonde\", \"Blonde\", …\n$ `Thumb Length` &lt;dbl&gt; 35, 40, 37, 42, 36, 45, 50, 57, 61, 38, 43, 62, 47, 49\n\nsummary(rawdata)\n\n    Height              Weight          Gender           Hair Color       \n Length:14          Min.   :  45.0   Length:14          Length:14         \n Class :character   1st Qu.:  55.0   Class :character   Class :character  \n Mode  :character   Median :  70.0   Mode  :character   Mode  :character  \n                    Mean   : 602.7                                        \n                    3rd Qu.:  90.0                                        \n                    Max.   :7000.0                                        \n                    NA's   :1                                             \n  Thumb Length  \n Min.   :35.00  \n 1st Qu.:38.50  \n Median :44.00  \n Mean   :45.86  \n 3rd Qu.:49.75  \n Max.   :62.00  \n                \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender `Hair Color` `Thumb Length`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                 &lt;dbl&gt;\n1 180        80 M      Brown                    35\n2 175        70 O      Brown                    40\n3 sixty      60 F      Blonde                   37\n4 178        76 F      Red                      42\n5 192        90 NA     Blonde                   36\n6 6          55 F      Blonde                   45\n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.0\n70\n90.00\n7000\n▇▁▁▁▁\n\n\nThumb Length\n0\n1.00\n45.86\n9.01\n35\n38.5\n44\n49.75\n62\n▇▅▅▁▅\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nThumb Length\n0\n1.00\n46.54\n9.00\n35\n40.00\n45\n50\n62\n▇▆▆▁▆\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nThumb Length\n0\n1.00\n46.54\n9.00\n35\n40.00\n45\n50\n62\n▇▆▆▁▆\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nThumb Length\n0\n1\n47.64\n9.34\n35\n41.0\n47\n53.5\n62\n▇▅▇▁▇\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n3\n6\n0\n3\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nThumb Length\n0\n1\n47.64\n9.34\n35\n41.0\n47\n53.5\n62\n▇▅▇▁▇\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n3\n6\n0\n3\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nThumb Length\n0\n1\n47.44\n8.35\n35\n42\n47\n50\n62\n▅▅▇▁▅\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Alexis Gonzalez Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Alexis Gonzalez Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  }
]